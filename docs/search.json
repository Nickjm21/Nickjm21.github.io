[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Nick Mesmer! I spend most of my time building AI-powered tools and exploring how maching learning can solve real-world problems. This blog is my corner of the internet to explore ideas, follow my curiosities, and share what I’m learning.\n\n\nBristol Myers Squibb | Enabling Functions AI/Data | December 2021 - present\nFermilab | Graduate Researcher | Sept 2019 - May 2021\n\n\n\nLewis University, Romeoville, IL M.S. in Physics | Sept 2019 - May 2021\nTaylor University | Upland, IN B.S. in Physics | Sept 2015 - May 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "Bristol Myers Squibb | Enabling Functions AI/Data | December 2021 - present\nFermilab | Graduate Researcher | Sept 2019 - May 2021"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Lewis University, Romeoville, IL M.S. in Physics | Sept 2019 - May 2021\nTaylor University | Upland, IN B.S. in Physics | Sept 2015 - May 2019"
  },
  {
    "objectID": "posts/wine_quality_prediction/index.html",
    "href": "posts/wine_quality_prediction/index.html",
    "title": "Predicting Wine Quality",
    "section": "",
    "text": "Can we predict wine quality from its chemical properties alone?\nIn my day job, I develop AI tools to boost productivity and efficiency across my organization. While this occasionally involves designing machine learning architectures, much of it revolves around orchestrating LLM APIs like OpenAI’s or Anthropic’s.\nCurious to deepen my understanding of how these models work under the hood, I’ve decided to start from the ground up—by implementing foundational learning algorithms from scratch.\nInitially, I considered using the MNIST dataset to predict handwritten digits. However, I wanted something a bit more engaging and relatable. That’s when I came across this Wine Quality Dataset (Cortez and Reis 2009) from the UCI Machine Learning Repository.\nThis dataset contains several chemical properties of wine samples and their corresponding quality ratings (on a scale from 1 to 10). It immediately struck me as a more interesting and real-world challenge: predicting something as subjective as wine quality from raw chemical data."
  },
  {
    "objectID": "posts/wine_quality_prediction/index.html#what-well-explore",
    "href": "posts/wine_quality_prediction/index.html#what-well-explore",
    "title": "Predicting Wine Quality",
    "section": "What We’ll Explore",
    "text": "What We’ll Explore\nIn this post, we’ll walk through:\n\nThe Dataset: Exploring 11 chemical features from 6,497 wine samples to uncover patterns beneath the surface.\nMachine Learning from Scratch: Implementing and comparing three models without using high-level ML libraries:\n\nSimple Linear Regression (as a baseline)\nLogistic Regression (to treat the problem as classification)\nA Neural Network (to introduce deep learning fundamentals)\n\nPractical Insights: Highlighting which chemical properties actually influence perceived wine quality.\n\nLet’s dive in and see what the data can reveal.\n\n# Import required libraries\nimport numpy as np                    \nimport pandas as pd\nfrom time import time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom ucimlrepo import fetch_ucirepo   # UCI Machine Learning Repository access\n\n\n# Set random seed for reproducibility across runs\nnp.random.seed(42)\n\n# Configure matplotlib for clean plots\nplt.style.use('default')\nplt.rcParams['figure.facecolor'] = 'white'\n\n# Fetch the wine quality dataset from UCI ML Repository\nprint(\"Loading Wine Quality dataset...\")\nwine_quality = fetch_ucirepo(id=186)  # ID 186 = Wine Quality dataset\nprint(\"Dataset loaded successfully!\") \n\nLoading Wine Quality dataset...\nDataset loaded successfully!"
  },
  {
    "objectID": "posts/wine_quality_prediction/index.html#data-exploration",
    "href": "posts/wine_quality_prediction/index.html#data-exploration",
    "title": "Predicting Wine Quality",
    "section": "Data Exploration",
    "text": "Data Exploration\nLet’s start by examining the structure and characteristics of our dataset. We’ll look at the features, their distributions, and understand what each variable represents in the context of wine quality assessment.\n\n# Extract features and target variable\nX = wine_quality.data.features \ny = wine_quality.data.targets\n\n# Display basic dataset information\nprint(\"Dataset Information:\")\nprint(f\"Number of samples: {len(X)}\")\nprint(f\"Number of features: {len(X.columns)}\")\n\nprint(\"\\n\\n\" + \"=\"*50)\nprint(\"Sample Data Preview:\")\nprint(\"=\"*50)\n\nprint(\"\\nFeature Data (first 5 samples):\")\ndisplay(X.head())\n\nprint(\"\\nTarget Data (first 5 samples):\")\ndisplay(y.head())\n\nprint(f\"\\nTarget distribution (quality ratings):\")\ndisplay(y['quality'].value_counts().sort_index())\n\nDataset Information:\nNumber of samples: 6497\nNumber of features: 11\n\n\n==================================================\nSample Data Preview:\n==================================================\n\nFeature Data (first 5 samples):\n\n\n\n\n\n\n\n\n\nfixed_acidity\nvolatile_acidity\ncitric_acid\nresidual_sugar\nchlorides\nfree_sulfur_dioxide\ntotal_sulfur_dioxide\ndensity\npH\nsulphates\nalcohol\n\n\n\n\n0\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n\n\n1\n7.8\n0.88\n0.00\n2.6\n0.098\n25.0\n67.0\n0.9968\n3.20\n0.68\n9.8\n\n\n2\n7.8\n0.76\n0.04\n2.3\n0.092\n15.0\n54.0\n0.9970\n3.26\n0.65\n9.8\n\n\n3\n11.2\n0.28\n0.56\n1.9\n0.075\n17.0\n60.0\n0.9980\n3.16\n0.58\n9.8\n\n\n4\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n\n\n\n\n\n\n\n\nTarget Data (first 5 samples):\n\n\n\n\n\n\n\n\n\nquality\n\n\n\n\n0\n5\n\n\n1\n5\n\n\n2\n5\n\n\n3\n6\n\n\n4\n5\n\n\n\n\n\n\n\n\nTarget distribution (quality ratings):\n\n\nquality\n3      30\n4     216\n5    2138\n6    2836\n7    1079\n8     193\n9       5\nName: count, dtype: int64\n\n\n\nFeature Descriptions\nUnderstanding the physicochemical properties that influence wine quality is essential to our analysis. Below is a breakdown of each feature in the dataset:\n\nChemical Properties\n\nFixed Acidity: Tartaric acid content (g/dm³)\n→ The primary acid in grapes; contributes to a wine’s tartness and balances sweetness.\nVolatile Acidity: Acetic acid content (g/dm³)\n→ High levels can lead to a vinegar-like taste, often considered a flaw.\nCitric Acid: Citric acid content (g/dm³)\n→ Occasionally added to enhance freshness, flavor, and balance.\npH: Acidity level (typically between 3 and 4)\n→ A lower pH indicates higher acidity, influencing taste and microbial stability.\n\n\n\nPreservation & Additives\n\nFree Sulfur Dioxide: Unbound SO₂ (mg/dm³)\n→ Acts as an antioxidant and antimicrobial agent; crucial for wine preservation.\nTotal Sulfur Dioxide: Combined free and bound SO₂ (mg/dm³)\n→ Total measure of sulfur dioxide used in production and preservation.\nSulphates: Sulfate content (g/dm³)\n→ A natural preservative that can enhance shelf life and protect against spoilage.\nChlorides: Chloride content (g/dm³)\n→ Influenced by mineral content and location; excessive levels may lead to off-flavors.\n\n\n\nStructural Components\n\nDensity: Wine density (g/cm³)\n→ Correlated with residual sugar and alcohol content; can indicate fermentation progress.\nResidual Sugar: Unfermented grape sugars (g/dm³)\n→ Contributes to the wine’s sweetness and mouthfeel.\nAlcohol: Alcohol percentage by volume\n→ A primary driver of wine character and body; often correlates with quality scores.\n\n\nNow that we understand what each feature represents, let’s visualize their distributions to get a better sense of how they vary across our dataset.\n\n\nCode\n# Create comprehensive feature distribution plots\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\nfig.suptitle('Wine Feature Distributions', fontsize=18, fontweight='bold', y=0.98)\n\n# Define units for each feature\nfeature_units = {\n    'fixed_acidity': r'(g/dm³)',\n    'volatile_acidity': r'(g/dm³)',\n    'citric_acid': r'(g/dm³)',\n    'residual_sugar': r'(g/dm³)',\n    'chlorides': r'(g/dm³)',\n    'free_sulfur_dioxide': r'(mg/dm³)',\n    'total_sulfur_dioxide': r'(mg/dm³)',\n    'density': r'(g/cm³)',\n    'pH': '',\n    'sulphates': r'(g/dm³)',\n    'alcohol': r'(% vol.)'\n}\n\n# Flatten axes for easier indexing\naxes = axes.flatten()\n\n# Create histograms for each feature\nfor i, feature in enumerate(X.columns):\n    ax = axes[i]\n    \n    # Create histogram with improved styling\n    n, bins, patches = ax.hist(X[feature], bins=30, alpha=0.7, \n                              color='steelblue', edgecolor='black', linewidth=0.5)\n    \n    # Calculate statistics\n    mean_val = X[feature].mean()\n    std_val = X[feature].std()\n    \n    # Add vertical lines for median\n    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, \n               label=f'Mean: {mean_val:.2f}')\n    \n    # Add statistical information as text\n    stats_text = f'μ = {mean_val:.2f}\\nσ = {std_val:.2f}\\nRange: [{X[feature].min():.2f}, {X[feature].max():.2f}]'\n    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,\n            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n    \n    # Format axis labels\n    unit = feature_units.get(feature, '')\n    feature_name = feature.replace(\"_\", \" \").title()\n    xlabel = f'{feature_name} {unit}' if unit else feature_name\n\n    # Customize the plot\n    ax.set_title(feature_name, fontweight='bold', fontsize=12)\n    ax.set_xlabel(xlabel, fontsize=10)\n    ax.set_ylabel('Frequency', fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.legend(fontsize=8)\n\n# Hide the unused subplot (12th position)\naxes[11].set_visible(False)\n\n# Adjust layout and display\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()\n\n# Print summary statistics\nprint(\"\\nFeature Summary Statistics:\")\nprint(\"=\" * 60)\ndisplay(X.describe().round(2))\n\n\n\n\n\n\n\n\n\n\nFeature Summary Statistics:\n============================================================\n\n\n\n\n\n\n\n\n\nfixed_acidity\nvolatile_acidity\ncitric_acid\nresidual_sugar\nchlorides\nfree_sulfur_dioxide\ntotal_sulfur_dioxide\ndensity\npH\nsulphates\nalcohol\n\n\n\n\ncount\n6497.00\n6497.00\n6497.00\n6497.00\n6497.00\n6497.00\n6497.00\n6497.00\n6497.00\n6497.00\n6497.00\n\n\nmean\n7.22\n0.34\n0.32\n5.44\n0.06\n30.53\n115.74\n0.99\n3.22\n0.53\n10.49\n\n\nstd\n1.30\n0.16\n0.15\n4.76\n0.04\n17.75\n56.52\n0.00\n0.16\n0.15\n1.19\n\n\nmin\n3.80\n0.08\n0.00\n0.60\n0.01\n1.00\n6.00\n0.99\n2.72\n0.22\n8.00\n\n\n25%\n6.40\n0.23\n0.25\n1.80\n0.04\n17.00\n77.00\n0.99\n3.11\n0.43\n9.50\n\n\n50%\n7.00\n0.29\n0.31\n3.00\n0.05\n29.00\n118.00\n0.99\n3.21\n0.51\n10.30\n\n\n75%\n7.70\n0.40\n0.39\n8.10\n0.06\n41.00\n156.00\n1.00\n3.32\n0.60\n11.30\n\n\nmax\n15.90\n1.58\n1.66\n65.80\n0.61\n289.00\n440.00\n1.04\n4.01\n2.00\n14.90\n\n\n\n\n\n\n\nAside: I can’t believe I only just learned about the pandas .describe() method. Incredibly helpful."
  },
  {
    "objectID": "posts/wine_quality_prediction/index.html#data-preprocessing",
    "href": "posts/wine_quality_prediction/index.html#data-preprocessing",
    "title": "Predicting Wine Quality",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore we can train any models, we need to prepare the dataset. This includes checking for missing values, standardizing our features, and splitting the data into training and testing sets.\n\nThe Data Quality Check\nFirst things first — let’s confirm that our dataset is complete. Missing values can disrupt training and lead to misleading results, so it’s important to address them up front.\n\n\nCode\n# Check for missing values in features\nmissing_features = X.isnull().sum()\nprint(\"Missing values in features:\")\nprint(missing_features)\nprint(f\"\\nTotal missing values: {missing_features.sum()}\")\n\n# Check for missing values in target\nmissing_target = y.isnull().sum()\nprint(f\"\\nMissing values in target: {missing_target.values[0]}\")\n\n# Summary\nif missing_features.sum() == 0 and missing_target.sum() == 0:\n    print(\"\\n✓ Excellent! No missing values found in the dataset.\")\nelse:\n    print(f\"\\n⚠ Warning: Found {missing_features.sum() + missing_target.sum()} missing values that need to be addressed.\")\n\n\nMissing values in features:\nfixed_acidity           0\nvolatile_acidity        0\ncitric_acid             0\nresidual_sugar          0\nchlorides               0\nfree_sulfur_dioxide     0\ntotal_sulfur_dioxide    0\ndensity                 0\npH                      0\nsulphates               0\nalcohol                 0\ndtype: int64\n\nTotal missing values: 0\n\nMissing values in target: 0\n\n✓ Excellent! No missing values found in the dataset.\n\n\n\n\nData Standardization\nExcellent, our dataset is complete! The next step is to standardize our features so they’re on a common scale.\nWhy do we do this?\nConsider the difference between alcohol percentage (typically 8–15%) and total sulfur dioxide (which can exceed 300 mg/dm³). Without standardization, features with larger numeric ranges can disproportionately influence our models — even if they’re not more important.\nThe Solution\nWe’ll use Z-score normalization, which transforms each feature to have a mean of 0 and a standard deviation of 1:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nThis ensures that all features contribute equally when we train our models.\nTrain-Test Split: We’ll also split our data into training (80%) and testing (20%) sets. This allows us to evaluate how well our models perform on unseen data.\n\n# Define train-test split ratio\nTRAIN_SIZE = 0.8\nTEST_SIZE = 1 - TRAIN_SIZE\n\nprint(f\"Data split: {TRAIN_SIZE*100:.0f}% training, {TEST_SIZE*100:.0f}% testing\")\n\n# Calculate standardization parameters from the full dataset\nX_means = X.mean()\nX_stds = X.std()\ny_means = y.mean()\ny_stds = y.std()\n\n# Apply z-score standardization: (x - mean) / std\n# This transforms features to have mean=0 and std=1\nX_norm = (X - X_means) / X_stds\ny_norm = (y - y_means) / y_stds\n\nprint(f\"\\nStandardization completed:\")\nprint(f\"Features - Mean: {X_norm.mean().mean():.2e}, Std: {X_norm.std().mean():.2f}\")\nprint(f\"Target - Mean: {y_norm.mean().values[0]:.2e}, Std: {y_norm.std().values[0]:.2f}\")\n\n# Split data into training and testing sets\nsplit_idx = int(TRAIN_SIZE * len(X_norm))\n\nX_train = X_norm.iloc[:split_idx]\nX_test = X_norm.iloc[split_idx:]\ny_train = y_norm.iloc[:split_idx]\ny_test = y_norm.iloc[split_idx:]\n\n# Ensure no samples are lost in the split\ndata_integrity = (len(X_norm) == len(X_train) + len(X_test)) and (len(y_norm) == len(y_train) + len(y_test))\nprint(f\"\\nNo data lost: {data_integrity}\")\n\n# Display dataset information in a clean format\nprint(\"\\nDataset Shapes:\")\nprint(f\"{'Set':&lt;10} {'Features':&lt;15} {'Target':&lt;15}\")\nprint(\"-\" * 40)\nprint(f\"{'Training':&lt;10} {str(X_train.shape):&lt;15} {str(y_train.shape):&lt;15}\")\nprint(f\"{'Testing':&lt;10} {str(X_test.shape):&lt;15} {str(y_test.shape):&lt;15}\")\nprint(f\"{'Total':&lt;10} {str(X_norm.shape):&lt;15} {str(y_norm.shape):&lt;15}\")\n\nData split: 80% training, 20% testing\n\nStandardization completed:\nFeatures - Mean: -9.23e-17, Std: 1.00\nTarget - Mean: -2.89e-16, Std: 1.00\n\nNo data lost: True\n\nDataset Shapes:\nSet        Features        Target         \n----------------------------------------\nTraining   (5197, 11)      (5197, 1)      \nTesting    (1300, 11)      (1300, 1)      \nTotal      (6497, 11)      (6497, 1)"
  },
  {
    "objectID": "posts/wine_quality_prediction/index.html#machine-learning-models",
    "href": "posts/wine_quality_prediction/index.html#machine-learning-models",
    "title": "Predicting Wine Quality",
    "section": "Machine Learning Models",
    "text": "Machine Learning Models\nNow for the fun part — building models to predict wine quality! I’ll implement three approaches, each increasing in complexity. By building them from scratch, we’ll gain an intuitive understanding of what’s happening under the hood.\n\n1. Simple Linear Regression (Single Feature)\nLet’s begin with the most basic model: predicting wine quality using just one feature, pH. This serves as a baseline and helps us explore whether a simple linear relationship exists between acidity and quality.\nWhy pH?\nFrom our earlier exploration, pH was one of the most normally distributed features and represents a core characteristic of wine. Starting with a single feature also simplifies the math and makes model behavior easier to interpret.\n\nThe Mathematical Model\nOur linear regression model is defined as:\n\\[\n\\hat{y} = \\theta_0 + \\theta_1 x\n\\]\nWhere \\(\\hat{y}\\) is the predicted wine quality (standardized), \\(x\\) is the feature value (standardized pH value), and \\(\\theta_i\\) is the model parameter.\n\n\nCost Function\nWe’ll use Mean Squared Error (MSE) to measure model performance:\n\\[\nJ(\\boldsymbol{\\theta}) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2\n\\]\nWhere \\(m\\) is the number of training samples, \\(y^{(i)}\\) is the true value, and \\(\\hat{y}^{(i)}\\) is the predicted value.\n\n\nParameter Optimization\nTo minimize the cost, we’ll apply gradient descent. The gradients of the cost function with respect to each parameter are:\n\\[\n\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})\n\\]\n\\[\n\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) x^{(i)}\n\\]\nWe update the parameters using the learning rate \\(\\alpha\\):\n\\[\n\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}\n\\]\nThis iterative process continues until convergence — or until we’ve reached a maximum number of iterations.\n\n\nclass LinearRegression:\n    \"\"\"\n    Simple Linear Regression implementation using gradient descent\n    \"\"\"\n    \n    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-10):\n        \"\"\"\n        Initialize the linear regression model\n        \n        Parameters:\n        - learning_rate: Step size for gradient descent\n        - max_iterations: Maximum number of training iterations\n        - tolerance: Convergence threshold (stop when cost change &lt; tolerance)\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.tolerance = tolerance\n        self.theta_0 = 0\n        self.theta_1 = 0\n        self.cost_history = []\n        \n    def cost_function(self, X, y):\n        \"\"\"\n        Calculate Mean Squared Error cost\n        \"\"\"\n        m = len(X)\n        predictions = self.theta_0 + self.theta_1 * X\n        return (1/(2*m)) * np.sum((predictions - y)**2)\n    \n    def fit(self, X, y):\n        \"\"\"Train the model using gradient descent\"\"\"\n        m = len(X)\n        \n        # Record initial cost\n        initial_cost = self.cost_function(X, y)\n        self.cost_history.append(initial_cost)\n        \n        print(f\"Starting training with initial cost: {initial_cost:.6f}\")\n        \n        for iteration in range(self.max_iterations):\n            # Forward pass: calculate predictions and errors\n            predictions = self.theta_0 + self.theta_1 * X\n            errors = predictions - y\n            \n            # Calculate gradients (partial derivatives of cost function)\n            theta_0_grad = (1/m) * np.sum(errors)\n            theta_1_grad = (1/m) * np.sum(errors * X)\n            \n            # Update parameters using gradient descent\n            self.theta_0 -= self.learning_rate * theta_0_grad\n            self.theta_1 -= self.learning_rate * theta_1_grad\n            \n            # Calculate new cost to track progress\n            current_cost = self.cost_function(X, y)\n            self.cost_history.append(current_cost)\n            \n            # Print progress every 100 iterations\n            if iteration % 100 == 0:\n                print(f\"Iteration {iteration:4d}: Cost = {current_cost:.6f}\")\n                \n            # Check for convergence (stop if cost change is very small)\n            if len(self.cost_history) &gt; 1:\n                cost_change = abs(self.cost_history[-2] - self.cost_history[-1])\n                if cost_change &lt; self.tolerance:\n                    print(f\"Converged at iteration {iteration} (cost change: {cost_change:.2e})\")\n                    break\n                    \n        final_cost = self.cost_history[-1]\n        print(f\"Training completed. Final cost: {final_cost:.6f}\")\n        return self\n    \n    def predict(self, X):\n        \"\"\"\n        Make predictions using the trained model\n        \"\"\"\n        return self.theta_0 + self.theta_1 * X\n\n# Prepare data for simple linear regression\nX_linear = X_train['pH'].to_numpy()\ny_linear = y_train.to_numpy().flatten()\n\nprint(\"Simple Linear Regression Training\")\nprint(\"=\" * 40)\nprint(f\"Feature: pH\")\nprint(f\"Training samples: {len(X_linear)}\")\nprint(f\"Learning rate: 0.01\")\nprint()\n\n# Train the model\nmodel_linear = LinearRegression(learning_rate=0.1, max_iterations=1000)\nmodel_linear.fit(X_linear, y_linear)\n\nprint(f\"\\nLearned parameters:\")\nprint(f\"θ_0 (intercept): {model_linear.theta_0:.6f}\")\nprint(f\"θ_1 (slope): {model_linear.theta_1:.6f}\")\n\nSimple Linear Regression Training\n========================================\nFeature: pH\nTraining samples: 5197\nLearning rate: 0.01\n\nStarting training with initial cost: 0.519063\nIteration    0: Cost = 0.518987\nConverged at iteration 70 (cost change: 8.29e-11)\nTraining completed. Final cost: 0.518638\n\nLearned parameters:\nθ_0 (intercept): -0.019141\nθ_1 (slope): 0.023184\n\n\n\n\nTraining Progress Visualization\nLet’s visualize how the cost function decreased during training to understand the convergence behavior.\n\n\nCode\n# Plot the cost history\ncost_array = model_linear.cost_history\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(len(cost_array)), cost_array, 'b-', linewidth=0.4, marker='o', markersize=2)\nplt.title('Cost Function History During Training', fontsize=14, fontweight='bold')\nplt.xlabel('Iteration', fontsize=12)\nplt.ylabel('Cost J(θ)', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.xlim(0, len(cost_array)-1)\n\nplt.tight_layout()\nplt.show()\n\n# Print some statistics about the cost reduction\ninitial_cost = cost_array[0]\nfinal_cost = cost_array[-1]\ncost_reduction = initial_cost - final_cost\npercent_reduction = (cost_reduction / initial_cost) * 100\n\n\n\n\n\n\n\n\n\n\n\nModel Evaluation and Visualization\nNow let’s evaluate our simple linear regression model using standard metrics and visualize the results. We’ll calculate the \\(R^2\\) score and create a comprehensive visualization of the model’s performance.\n\n\nCode\n# Plot the linear regression line with the data\nplt.figure(figsize=(12, 8))\n\n# Plot the training data points\nplt.scatter(X_linear, y_linear, alpha=0.6, color='lightblue', s=30, label='Training Data')\n\n# Create a range of x values for the regression line\nx_range = np.linspace(X_linear.min(), X_linear.max(), 100)\ny_pred = model_linear.theta_0 + model_linear.theta_1 * x_range\n\n# Plot the regression line\nplt.plot(x_range, y_pred, 'r-', linewidth=3, label=fr'Regression Line: $\\hat{{y}}$ = {model_linear.theta_0:.3f} + {model_linear.theta_1:.3f}x')\n\n# Customize the plot\nplt.title('Single Variable Linear Regression: pH vs Wine Quality', fontsize=14, fontweight='bold')\nplt.xlabel('pH (standardized)', fontsize=12)\nplt.ylabel('Wine Quality (standardized)', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=11)\n\n# Add R^2 score\nr_squared = 1 - (np.sum((y_linear - (model_linear.theta_0 + model_linear.theta_1 * X_linear))**2) / \n                 np.sum((y_linear - np.mean(y_linear))**2))\n\nplt.text(0.05, 0.95, f'$R^2$ = {r_squared:.4f}', \n         transform=plt.gca().transAxes, fontsize=11, \n         bbox=dict(boxstyle='round', facecolor='white'),\n         verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWith an \\(R^2\\) score of just 0.0005, our simple linear regression model is essentially useless for predicting wine quality. In other words, wine quality isn’t determined by pH alone. This intuitively makes sense.\nWhat Does This Tell Us? - The relationship between pH and wine quality is incredibly weak. - Wine quality likely depends on the interaction of many chemical factors. - We need a more sophisticated, multivariate approach.\nThis “failure” is actually valuable. It reinforces the complexity of the problem: wine quality isn’t something you can capture with a single variable. Let’s see how a more capable model performs.\n\n\n\n\n2. Logistic Regression\nInstead of predicting an exact score — which is subjective and often noisy — let’s simplify the problem:\n\nIs this wine high quality or not?\n\nWe’ll define:\n\nHigh Quality: Wine quality \\(&gt;= 7 \\rightarrow\\) label = 1\n\nLow Quality: Wine quality \\(&lt; 7 \\rightarrow\\) label = 0\n\nThis framing allows us to use logistic regression, a foundational classification algorithm.\n\nThe Mathematical Model\nLogistic regression applies the sigmoid function to the linear combination of inputs, transforming predictions into probabilities between 0 and 1:\n\\[\n\\hat{y} = \\sigma(X\\boldsymbol{\\theta}) = \\frac{1}{1 + e^{-X\\boldsymbol{\\theta}}}\n\\]\nWhere \\(X\\) is our feature matrix (all 11 chemical properties), \\(\\boldsymbol{\\theta}\\) is the parameter vector we’re learning, and \\(\\sigma\\) is the sigmoid activation function.\n\n\nParameter Optimization\nTo train this model, we’ll use Binary Cross-Entropy Loss:\n\\[\nJ(\\boldsymbol{\\theta}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n\\]\nWhat’s continually impressed me while learning about various machine learning architectures, is how simple the gradients often turn out to be. In this case, the gradient of the loss function is:\n\\[\n\\nabla J(\\boldsymbol{\\theta}) = \\frac{1}{m} X^T (\\hat{y} - y)\n\\]\nThis is almost identical to the gradient for linear regression — the only difference is that \\(\\hat{y}\\) now comes from the sigmoid function.\n\n\nclass LogisticRegression:\n    \"\"\"\n    Logistic Regression implementation using gradient descent\n    \"\"\"\n    \n    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.tolerance = tolerance\n        self.theta = None\n        self.cost_history = []\n        \n    def sigmoid(self, z):\n        \"\"\"Sigmoid activation function\"\"\"\n        return 1 / (1 + np.exp(-z))\n    \n    def cost_function(self, X, y):\n        \"\"\"Calculate Binary Cross-Entropy cost\"\"\"\n        m = len(X)\n        z = X @ self.theta\n        h = self.sigmoid(z)\n        \n        cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n        return cost\n    \n    def fit(self, X, y):\n        \"\"\"Train the model using gradient descent\"\"\"\n        m, n = X.shape\n        \n        # Initialize parameters\n        self.theta = np.random.normal(0, 0.01, (n, 1))\n        \n        # Record initial cost\n        initial_cost = self.cost_function(X, y)\n        self.cost_history.append(initial_cost)\n        \n        print(f\"Starting logistic regression training...\")\n        print(f\"Initial cost: {initial_cost:.6f}\")\n        print(f\"Features: {n}, Samples: {m}\")\n        print()\n        \n        for iteration in range(self.max_iterations):\n            # Forward pass\n            z = X @ self.theta\n            h = self.sigmoid(z)\n            \n            # Calculate gradient\n            gradient = (1/m) * X.T @ (h - y)\n            \n            # Update parameters\n            self.theta -= self.learning_rate * gradient\n            \n            # Calculate new cost\n            current_cost = self.cost_function(X, y)\n            self.cost_history.append(current_cost)\n            \n            # Print progress\n            if iteration % 100 == 0:\n                print(f\"Iteration {iteration:4d}: Cost = {current_cost:.6f}\")\n                \n            # Check for convergence\n            if len(self.cost_history) &gt; 1:\n                cost_change = abs(self.cost_history[-2] - self.cost_history[-1])\n                if cost_change &lt; self.tolerance:\n                    print(f\"Converged at iteration {iteration} (cost change: {cost_change:.2e})\")\n                    break\n                    \n        final_cost = self.cost_history[-1]\n        print(f\"Training completed. Final cost: {final_cost:.6f}\")\n        return self\n    \n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities\"\"\"\n        z = X @ self.theta\n        return self.sigmoid(z)\n    \n    def predict(self, X, threshold=0.5):\n        \"\"\"Make binary predictions\"\"\"\n        probabilities = self.predict_proba(X)\n        return (probabilities &gt;= threshold).astype(int)\n\n# Data preparation for logistic regression\nprint(\"Preparing data for logistic regression...\")\nprint(\"=\" * 45)\n\n# Transform target to binary classification (quality &gt;= 7 = 1, else 0)\nquality_threshold = 7\ny_binary = (y['quality'] &gt;= quality_threshold).astype(int).values.reshape(-1, 1)\n\n# Split binary target\ny_train_binary = y_binary[:len(X_train)]\ny_test_binary = y_binary[len(X_train):]\n\n# Use standardized features\nX_train_logistic = X_train.values\nX_test_logistic = X_test.values\n\n# Display class distribution\ntrain_positive = np.sum(y_train_binary)\ntrain_total = len(y_train_binary)\ntest_positive = np.sum(y_test_binary)\ntest_total = len(y_test_binary)\n\nprint(f\"Quality threshold: {quality_threshold}\")\nprint(f\"Training set: {train_positive}/{train_total} ({train_positive/train_total*100:.1f}%) high quality\")\nprint(f\"Test set: {test_positive}/{test_total} ({test_positive/test_total*100:.1f}%) high quality\")\nprint()\n\n# Train logistic regression model\nmodel_logistic = LogisticRegression(learning_rate=0.2, max_iterations=250)\nmodel_logistic.fit(X_train_logistic, y_train_binary)\nprint(\"=\" * 45)\n\nPreparing data for logistic regression...\n=============================================\nQuality threshold: 7\nTraining set: 1030/5197 (19.8%) high quality\nTest set: 247/1300 (19.0%) high quality\n\nStarting logistic regression training...\nInitial cost: 0.695249\nFeatures: 11, Samples: 5197\n\nIteration    0: Cost = 0.679790\nIteration  100: Cost = 0.588730\nIteration  200: Cost = 0.587354\nTraining completed. Final cost: 0.587092\n=============================================\n\n\n\n\nCode\n# Visualize training progress\nplt.figure(figsize=(12, 5))\n\n# Plot cost history\nplt.plot(range(len(model_logistic.cost_history)), model_logistic.cost_history, 'b-', linewidth=2)\nplt.title('Logistic Regression Training Progress', fontweight='bold')\nplt.xlabel('Iteration')\nplt.ylabel('Cost (Binary Cross-Entropy)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Training completed in {len(model_logistic.cost_history)-1} iterations\")\nprint(f\"Final training cost: {model_logistic.cost_history[-1]:.6f}\")\n\n\n\n\n\n\n\n\n\nTraining completed in 250 iterations\nFinal training cost: 0.587092\n\n\n\n\nModel Evaluation:\nSince this model is more robust than our single-feature linear regression, it’s worth taking a closer look at how well it performs. Instead of relying solely on accuracy, we’ll evaluate it using a set of standard classification metrics:\n\nAccuracy:\nThe proportion of correct predictions overall. This gives a general sense of model performance but can be misleading on imbalanced datasets.\nPrecision:\nThe percentage of predicted high-quality wines that are actually high quality. High precision means the model makes few false positives.\nRecall:\nThe percentage of actual high-quality wines that were correctly identified. High recall means the model catches most of the positives.\nF1 Score:\nThe harmonic mean of precision and recall. It balances the trade-off between the two, especially useful when class distributions are uneven.\n\nThese metrics help us evaluate how well the model performs across different dimensions of classification quality — not just how often it gets things right, but what kind of mistakes it makes.\n\n\nCode\ndef evaluate_model(model, X_train, y_train, X_test, y_test, loss_history=None):\n    \"\"\"\n    Evaluate a classification or neural network model and return metrics.\n    \n    Parameters:\n    - model: A trained model with predict() and predict_proba() methods.\n    - X_train, y_train: Training data and labels.\n    - X_test, y_test: Testing data and labels.\n    - loss_history (optional): List of loss values for neural networks.\n    \n    Returns:\n    - dict of evaluation metrics.\n    \"\"\"\n    # Make predictions\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    y_train_proba = model.predict_proba(X_train)\n    y_test_proba = model.predict_proba(X_test)\n\n    # Metrics\n    def accuracy(y_true, y_pred):\n        return np.mean(y_true.flatten() == y_pred.flatten())\n\n    def precision(y_true, y_pred):\n        tp = np.sum((y_true == 1) & (y_pred == 1))\n        fp = np.sum((y_true == 0) & (y_pred == 1))\n        return tp / (tp + fp) if (tp + fp) &gt; 0 else 0.0\n\n    def recall(y_true, y_pred):\n        tp = np.sum((y_true == 1) & (y_pred == 1))\n        fn = np.sum((y_true == 1) & (y_pred == 0))\n        return tp / (tp + fn) if (tp + fn) &gt; 0 else 0.0\n\n    def f1_score(prec, rec):\n        return 2 * (prec * rec) / (prec + rec) if (prec + rec) &gt; 0 else 0.0\n\n    # Compute metrics\n    train_acc = accuracy(y_train, y_train_pred)\n    test_acc = accuracy(y_test, y_test_pred)\n\n    train_prec = precision(y_train, y_train_pred)\n    test_prec = precision(y_test, y_test_pred)\n\n    train_rec = recall(y_train, y_train_pred)\n    test_rec = recall(y_test, y_test_pred)\n\n    train_f1 = f1_score(train_prec, train_rec)\n    test_f1 = f1_score(test_prec, test_rec)\n\n    results = {\n        'train_accuracy': train_acc,\n        'test_accuracy': test_acc,\n        'train_precision': train_prec,\n        'test_precision': test_prec,\n        'train_recall': train_rec,\n        'test_recall': test_rec,\n        'train_f1': train_f1,\n        'test_f1': test_f1,\n        'y_train_pred': y_train_pred,\n        'y_test_pred': y_test_pred,\n        'y_train_proba': y_train_proba,\n        'y_test_proba': y_test_proba\n    }\n\n    # Added after coding the neural network\n    if loss_history is not None:\n        recent_loss = np.mean(loss_history[-100:]) if len(loss_history) &gt;= 100 else loss_history[-1]\n        results.update({\n            'final_loss': loss_history[-1],\n            'avg_recent_loss': recent_loss,\n            'epochs_trained': len(loss_history),\n            'loss_history': loss_history\n        })\n\n    return results\n\n\n# Evaluate logistic regression model\nprint(\"Evaluating Logistic Regression Model...\")\nprint(\"=\" * 45)\n\nresults_logistic = evaluate_model(\n    model_logistic, X_train_logistic, y_train_binary, X_test_logistic, y_test_binary\n)\n\nprint(\"Classification Performance Summary:\")\nprint(\"-\" * 45)\nprint(f\"{'Metric':&lt;15} {'Training':&lt;12} {'Test':&lt;12}\")\nprint(\"-\" * 45)\nprint(f\"{'Accuracy':&lt;15} {results_logistic['train_accuracy']:&lt;12.4f} {results_logistic['test_accuracy']:&lt;12.4f}\")\nprint(f\"{'Precision':&lt;15} {results_logistic['train_precision']:&lt;12.4f} {results_logistic['test_precision']:&lt;12.4f}\")\nprint(f\"{'Recall':&lt;15} {results_logistic['train_recall']:&lt;12.4f} {results_logistic['test_recall']:&lt;12.4f}\")\nprint(f\"{'F1-Score':&lt;15} {results_logistic['train_f1']:&lt;12.4f} {results_logistic['test_f1']:&lt;12.4f}\")\n\n\nEvaluating Logistic Regression Model...\n=============================================\nClassification Performance Summary:\n---------------------------------------------\nMetric          Training     Test        \n---------------------------------------------\nAccuracy        0.7129       0.4623      \nPrecision       0.3900       0.2408      \nRecall          0.7951       0.8502      \nF1-Score        0.5233       0.3753      \n\n\n\n\nResults\nLooking at the results, I’m honestly pleasantly surprised. While a test accuracy of 46% may seem low at first glance, it’s a significant improvement over our linear regression baseline. To me, this suggests that there is genuinely a relationship between the chemical properties of the wines and their perceived quality.\nWhat’s Working Well\n\nHigh Recall (85%): We’re successfully identifying most high-quality wines — and that’s important. In real-world terms, we’d rather flag too many good wines than miss the truly great ones.\nTraining vs. Test Performance: While there’s a performance gap, it’s not extreme. This indicates the model is generalizing reasonably well and not drastically overfitting.\n\nWhat Needs Work\n\nLow Precision (24%): We’re making a lot of false positives — predicting wines are high quality when they’re not. This means the model is overconfident.\nModerate Accuracy: While better than chance, the overall accuracy leaves room for improvement. Clearly, the task is more complex than our current model can capture.\n\nThe Bottom Line\nOur logistic regression model is overly optimistic, but it’s doing a decent job at identifying most truly great wines. That’s a promising result and it motivates us to go deeper.\nLet’s investigate which chemical features are actually driving these predictions.\n\n\nCode\n# Feature importance\nfeature_names = X_train.columns\ncoefficients = model_logistic.theta.flatten()\nimportance = np.abs(coefficients)\nsorted_idx = np.argsort(importance)[::-1]\n\nplt.figure(figsize=(12, 5))\nplt.barh(range(len(feature_names)), importance[sorted_idx], color='steelblue')\nplt.yticks(range(len(feature_names)), [feature_names[i] for i in sorted_idx])\nplt.title('Feature Importance (|Coefficient|)', fontweight='bold')\nplt.xlabel('Absolute Coefficient Value')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n# Print top influential features\nprint(\"Top 5 Most Influential Features:\")\nprint(\"=\" * 40)\nfor i, idx in enumerate(sorted_idx[:5]):\n    feature = feature_names[idx]\n    coef = coefficients[idx]\n    print(f\"{i+1}. {feature:&lt;20} (coef: {coef:+.4f})\")\n\n\n\n\n\n\n\n\n\nTop 5 Most Influential Features:\n========================================\n1. alcohol              (coef: +0.7083)\n2. residual_sugar       (coef: +0.4670)\n3. density              (coef: -0.4646)\n4. sulphates            (coef: +0.2703)\n5. total_sulfur_dioxide (coef: -0.2338)\n\n\n\n\nInterpreting Feature Importance\nFrom the plot above, it’s clear that alcohol is by far the most influential feature in predicting whether a wine is classified as high quality. This aligns with intuition — alcohol content often correlates with flavor intensity and mouthfeel, which may contribute to higher quality scores.\n\n\nSurprises\nAfter alcohol, residual sugar and density are the next most influential features. These, along with alcohol, are the three structural features of our dataset. It’s interesting that they play the largest roles in determining the overall quality of the wines.\nSugar and density are often linked — higher sugar content typically increases a wine’s density — and both contribute to its body and perceived richness.\nHowever, the coefficients tell an interesting story:\n\nResidual sugar has a positive coefficient: higher sugar levels increase the likelihood of being classified as high quality.\n\nDensity, despite its correlation with sugar, has a negative coefficient: higher density reduces the likelihood.\n\nWhat Could Explain This?\nHere are a few possible interpretations:\n\nAlcohol vs. Sugar Tradeoff: Higher-density wines may also result from unfermented sugars — which can indicate a lower alcohol level. Since alcohol is strongly associated with higher quality in this model, density may be acting as a proxy for lower alcohol, hence the negative sign.\nPerception of Balance: Wines that are dense but not sweet may be perceived as heavy or cloying. In contrast, wines with balanced residual sugar and lower density may be seen as more elegant or refined.\nNonlinearity: It’s possible that the relationship between density and wine quality isn’t truly linear. A logistic regression can’t capture this, so it may incorrectly assign a negative weight to a feature that actually has a more complex relationship with the outcome.\n\nThis is a great example of how model interpretation isn’t always straightforward — and why exploring deeper models or visualizing feature interactions can uncover more nuanced relationships.\n\n\nA Quick Detour: Density vs. Alcohol\nI can’t help but further explore this — let’s visualize the relationship directly.\n\n\nCode\n#Plot density vs alcohol\ndensity = X[X['density'] &lt; 1.01]['density'].values # There's two wines with oddly high densities. Removing to make the plot look nicer\nalcohol = X[X['density'] &lt; 1.01]['alcohol'].values\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x=density, y=alcohol, alpha=0.5)\nplt.title('Density vs Alcohol')\nplt.xlabel('Density')\nplt.ylabel('Alcohol')\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see, there’s a clear inverse linear relationship between density and alcohol. In general, wines with higher density tend to have lower alcohol content.\nThis supports the idea that density is acting as a proxy for incomplete fermentation. During fermentation, sugar is converted into alcohol — so wines that are both dense and low in alcohol may simply be under-fermented or sweeter styles that the model associates with lower quality.\nIn contrast, alcohol is a strong positive predictor of quality in our model. So if the model sees high density and low alcohol, it logically adjusts the density coefficient to be negative in order to counteract the quality signal from residual sugar.\nThis kind of interaction — where two correlated features pull in opposite directions — is common in linear models and one reason why feature interpretation requires care. It’s not always about direct relationships, but about how the model distributes “credit” when multiple features overlap in what they explain.\n\n\n\n3. Neural Network\nNow for the most sophisticated model — let’s build a neural network. This is where things get exciting (and a bit more complex). Neural networks offer far more flexibility than linear models, giving them the power to learn intricate patterns that simpler models might miss.\n\nThe Architecture\nTo start, I’m keeping things intentionally simple:\n\nInput Layer: 11 neurons (one for each standardized chemical feature)\nHidden Layer: 11 neurons\n\nMirroring the input size\n\nOutput Layer: 1 neuron\n\nOutputs a probability between 0 and 1, representing the likelihood that a wine is high quality.\n\n\nThis architecture gives us just enough capacity to model interactions between features without overcomplicating things. Later, we can experiment with deeper networks or regularization techniques to fine-tune performance.\n\nclass NeuralNetwork:\n    \"\"\"\n    Single hidden layer neural network implementation\n    \"\"\"\n    \n    def __init__(self, learning_rate=0.1, max_epochs=5000, tolerance=0.00001):\n        self.learning_rate = learning_rate\n        self.max_epochs = max_epochs\n        self.tolerance = tolerance\n        self.theta_1 = None\n        self.theta_2 = None\n        self.loss_history = []\n        self.is_trained = False\n        \n    def _sigmoid(self, z):\n        \"\"\"Sigmoid activation function\"\"\"\n        return 1 / (1 + np.exp(-z))\n    \n    def _bce_loss(self, y, y_hat, eps=1e-8):\n        \"\"\"Binary cross-entropy loss\"\"\"\n        y_hat = np.clip(y_hat, eps, 1 - eps)\n        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n    \n    def fit(self, X, y, verbose=False):\n        \"\"\"\n        Train the neural network\n        \n        Parameters:\n        X : array-like, shape (m, n) - training features\n        y : array-like, shape (m, 1) - training targets (binary)\n        \"\"\"\n        # Prepare data\n        x = X.copy()\n        m, n = x.shape\n        x = np.hstack([np.ones((m, 1)), x])  # Add bias\n        \n        # Initialize weights\n        #self.theta_1 = np.random.normal(0, 0.01, size=(n+1, n))\n        #self.theta_2 = np.random.normal(0, 0.01, size=(n+1, 1))\n        self.theta_1 = np.random.randn(n+1, n) * np.sqrt(2 / (n+1))\n        self.theta_2 = np.random.randn(n+1, 1) * np.sqrt(2 / (n+1))\n        \n        self.loss_history = []\n        \n        if verbose:\n            print(f\"Starting Neural Network training...\")\n            print(f\"Samples: {m}, Learning rate: {self.learning_rate}\")\n            print()\n        \n        # Training loop\n        for i in range(self.max_epochs):\n            # Forward pass\n            z_2 = x @ self.theta_1\n            a_2 = self._sigmoid(z_2)\n            a_2 = np.hstack([np.ones((m, 1)), a_2])  # Add bias\n            \n            z_3 = a_2 @ self.theta_2\n            a_3 = self._sigmoid(z_3)\n            \n            # Calculate loss\n            loss = self._bce_loss(y, a_3)\n            self.loss_history.append(loss)\n            \n            # Print progress\n            if verbose:\n                if i % 1000 == 0:\n                    print(f\"Epoch {i:4d}: Loss = {loss:.6f}\")\n            \n            # Check for convergence\n            if len(self.loss_history) &gt; 1:\n                loss_change = abs(self.loss_history[-2] - self.loss_history[-1])\n                if loss_change &lt; self.tolerance:\n                    print(f\"Converged at epoch {i} (loss change: {loss_change:.2e})\")\n                    break\n            \n            # Backward pass - BCE loss with sigmoid\n            delta_3 = a_3 - y\n            grad_theta_2 = (a_2.T @ delta_3) / m\n            \n            delta_2_full = delta_3 @ self.theta_2.T\n            delta_2 = delta_2_full[:, 1:]\n            sigmoid_grad = a_2[:, 1:] * (1 - a_2[:, 1:])\n            delta_2 *= sigmoid_grad  # element-wise\n            \n            grad_theta_1 = (x.T @ delta_2) / m\n            \n            # Gradient descent\n            self.theta_1 -= self.learning_rate * grad_theta_1\n            self.theta_2 -= self.learning_rate * grad_theta_2\n        \n        self.is_trained = True\n        final_loss = self.loss_history[-1]\n        if verbose:\n            print(f\"Training completed. Final loss: {final_loss:.6f}\")\n            print(\"=\" * 50)\n        return self\n    \n    def predict_proba(self, X):\n        \"\"\"\n        Predict class probabilities\n        \n        Parameters:\n        X : array-like, shape (m, n) - input features\n        \n        Returns:\n        probabilities : array-like, shape (m, 1) - predicted probabilities\n        \"\"\"\n        if not self.is_trained:\n            raise ValueError(\"Model must be trained before making predictions\")\n        \n        x = X.copy()\n        m, n = x.shape\n        x = np.hstack([np.ones((m, 1)), x])  # Add bias\n        \n        # Forward pass\n        z_2 = x @ self.theta_1\n        a_2 = self._sigmoid(z_2)\n        a_2 = np.hstack([np.ones((m, 1)), a_2])  # Add bias\n        \n        z_3 = a_2 @ self.theta_2\n        a_3 = self._sigmoid(z_3)\n        \n        return a_3\n    \n    def predict(self, X, threshold=0.5):\n        \"\"\"\n        Make binary predictions\n        \n        Parameters:\n        X : array-like, shape (m, n) - input features\n        threshold : float - decision threshold (default: 0.5)\n        \n        Returns:\n        predictions : array-like, shape (m, 1) - binary predictions\n        \"\"\"\n        probabilities = self.predict_proba(X)\n        return (probabilities &gt;= threshold).astype(int)\n\nNow let’s run and evaluate it using the same function as before.\n\n# Create and train the neural network\nmodel_nn = NeuralNetwork(learning_rate=0.1, max_epochs=5000, tolerance=1e-10)\nmodel_nn.fit(X_train_logistic, y_train_binary,verbose=True)\n\n# Evaluate the neural network model\n\nresults_nn = evaluate_model(\n    model_nn, X_train_logistic, y_train_binary, X_test_logistic, y_test_binary, model_nn.loss_history\n)\n\nStarting Neural Network training...\nSamples: 5197, Learning rate: 0.1\n\nEpoch    0: Loss = 0.775274\nEpoch 1000: Loss = 0.382044\nEpoch 2000: Loss = 0.373464\nEpoch 3000: Loss = 0.370568\nEpoch 4000: Loss = 0.368550\nTraining completed. Final loss: 0.366776\n==================================================\n\n\n\n\nCode\n# Print statements to make the output look nice\nprint(\"Neural Network Performance Summary:\")\nprint(\"-\" * 45)\nprint(f\"{'Metric':&lt;15} {'Training':&lt;12} {'Test':&lt;12}\")\nprint(\"-\" * 45)\nprint(f\"{'Accuracy':&lt;15} {results_nn['train_accuracy']:&lt;12.4f} {results_nn['test_accuracy']:&lt;12.4f}\")\nprint(f\"{'Precision':&lt;15} {results_nn['train_precision']:&lt;12.4f} {results_nn['test_precision']:&lt;12.4f}\")\nprint(f\"{'Recall':&lt;15} {results_nn['train_recall']:&lt;12.4f} {results_nn['test_recall']:&lt;12.4f}\")\nprint(f\"{'F1-Score':&lt;15} {results_nn['train_f1']:&lt;12.4f} {results_nn['test_f1']:&lt;12.4f}\")\n\nprint(f\"\\nTraining Information:\")\nprint(\"-\" * 45)\nprint(f\"{'Final Loss:':&lt;20} {results_nn['final_loss']:.6f}\")\nprint(f\"{'Epochs Trained:':&lt;20} {results_nn['epochs_trained']}\")\nprint(f\"{'Avg Recent Loss:':&lt;20} {results_nn['avg_recent_loss']:.6f}\")\n\n\nNeural Network Performance Summary:\n---------------------------------------------\nMetric          Training     Test        \n---------------------------------------------\nAccuracy        0.8311       0.7831      \nPrecision       0.6203       0.4314      \nRecall          0.3806       0.4453      \nF1-Score        0.4717       0.4382      \n\nTraining Information:\n---------------------------------------------\nFinal Loss:          0.366776\nEpochs Trained:      5000\nAvg Recent Loss:     0.366860\n\n\n\n\nResults\nA 78% accuracy on our test set! That’s quite the improvement over logistic regression. Our precision, recall, and f1 score is still quite low but the NN is performing better than our logistic regression model as expected.\n\n\nSurprises\nOne thing I’ve noticed while experimenting with this neural network is how sensitive the performance is to the number of epochs. This begs the question: How long should we train our model?\nLet’s dive a little deeper and systematically explore this relationship to find the optimal training duration.\n\n\nCode\n#Plot the f1 score as a function of the number of epochs\n\nepoch_options = [100, 200, 500, 1000, 2000, 5000, 10000, 15000]\nf1_scores = []\n\nfor epoch in epoch_options:\n    model_nn = NeuralNetwork(tolerance = 0, max_epochs=epoch)\n    model_nn.fit(X_train, y_train_binary, verbose=False)\n    results_nn = evaluate_model(model_nn, X_train, y_train_binary, X_test, y_test_binary)\n    f1_scores.append(results_nn['test_f1'])\n\n\nsns.set(style=\"whitegrid\", palette=\"colorblind\", font_scale=1.2)\n\nplt.figure(figsize=(10, 6))\nplt.plot(epoch_options, f1_scores)\nplt.xlabel('Epochs')\nplt.ylabel('F1 Score')\nplt.title('F1 Score vs Number of Epochs')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe F1 score may still be improving, so let’s test longer training durations. While we’re at it, let’s also see how the other performance metrics change. Additionally, I’ll also examine the relationship between training time and epochs to understand the computational cost.\n\n\nCode\n#Plot the f1 score as a function of the number of epochs\n\nepoch_options = [100, 200, 500, 1000, 2000, 5000, 10000, 15000, 20000, 30000, 50000, 100000]\ntime_taken = []\n\nf1_scores = []\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\n\nfor epoch in epoch_options:\n    \n    model_nn = NeuralNetwork(tolerance = 0, learning_rate = 0.2, max_epochs=epoch)\n    start_time = time()\n    model_nn.fit(X_train, y_train_binary, verbose=False)\n    end_time = time()\n    time_taken.append(end_time - start_time)\n    results_nn = evaluate_model(model_nn, X_train, y_train_binary, X_test, y_test_binary)\n\n    f1_scores.append(results_nn['test_f1'])\n    accuracy_scores.append(results_nn['test_accuracy'])\n    precision_scores.append(results_nn['test_precision'])\n    recall_scores.append(results_nn['test_recall'])\n\n\n# Plot the results\n\n# --- Plot 1: Time Taken vs Epochs ---\nplt.figure(figsize=(10, 5))\nplt.plot(epoch_options, time_taken, marker='o', linestyle='-', linewidth=2, color='teal')\nplt.xlabel('Epochs')\nplt.ylabel('Time Taken (s)')\nplt.title('Training Time vs Number of Epochs')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# --- Plot 2: Evaluation Metrics vs Epochs ---\nplt.figure(figsize=(10, 6))\nplt.plot(epoch_options, f1_scores, marker='o', label='F1 Score')\nplt.plot(epoch_options, accuracy_scores, marker='s', label='Accuracy')\nplt.plot(epoch_options, precision_scores, marker='^', label='Precision')\nplt.plot(epoch_options, recall_scores, marker='D', label='Recall')\nplt.xlabel('Epochs')\nplt.ylabel('Score')\nplt.title('Model Metrics vs Number of Epochs')\nplt.legend(title='Metrics', loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\nTraining Time vs. Epochs: As expected, training time scales linearly with epochs. This makes sense as each epoch requires one complete forward and backward pass through the entire dataset.\nPerformance Metrics: - The F1 score actually seems to peak around 15,000 epochs and then begins to decline steadily. - The accuracy is highest at 100 epochs (roughly 80%). Recall that 80% of our wine is “low quality”. I’m guessing that this model is just predicting low quality for all wines. This is why other metrics are valuable to consider. - Precision and recall both follow the same trend as the F1 score since the F1 score is the harmonic mean of precision and recall.\nLearning for the Future: There are more sophisticated techniques for determining optimal training duration (early stopping, learning rate scheduling, etc.), but this systematic approach gives us valuable insights into our model’s behavior. Additionally, in retrospect, designing the NN class to output weights at sub-epoch intervals would have been more computationally efficient than retraining each time.\nLet’s use the optimal 15,000-epoch model for our final analysis\n\nepoch_max_f1 = epoch_options[f1_scores.index(max(f1_scores))]\nprint(f\"Number of epochs with max F1 score: {epoch_max_f1}\")\n\n#Grab the model with the highest F1 score\nmodel_nn = NeuralNetwork(tolerance = 0, learning_rate = 0.2, max_epochs=epoch_max_f1)\nmodel_nn.fit(X_train, y_train_binary)\nresults_nn = evaluate_model(model_nn, X_train, y_train_binary, X_test, y_test_binary, model_nn.loss_history)\n\nNumber of epochs with max F1 score: 50000\n\n\n\n\nCode\n# Evaluation\nresults_nn = evaluate_model(\n    model_nn, X_train_logistic, y_train_binary, X_test_logistic, y_test_binary, model_nn.loss_history\n)\n\nprint(\"Neural Network Performance Summary:\")\nprint(\"-\" * 45)\nprint(f\"{'Metric':&lt;15} {'Training':&lt;12} {'Test':&lt;12}\")\nprint(\"-\" * 45)\nprint(f\"{'Accuracy':&lt;15} {results_nn['train_accuracy']:&lt;12.4f} {results_nn['test_accuracy']:&lt;12.4f}\")\nprint(f\"{'Precision':&lt;15} {results_nn['train_precision']:&lt;12.4f} {results_nn['test_precision']:&lt;12.4f}\")\nprint(f\"{'Recall':&lt;15} {results_nn['train_recall']:&lt;12.4f} {results_nn['test_recall']:&lt;12.4f}\")\nprint(f\"{'F1-Score':&lt;15} {results_nn['train_f1']:&lt;12.4f} {results_nn['test_f1']:&lt;12.4f}\")\n\nprint(f\"\\nTraining Information:\")\nprint(\"-\" * 45)\nprint(f\"{'Final Loss:':&lt;20} {results_nn['final_loss']:.6f}\")\nprint(f\"{'Epochs Trained:':&lt;20} {results_nn['epochs_trained']}\")\nprint(f\"{'Avg Recent Loss:':&lt;20} {results_nn['avg_recent_loss']:.6f}\")\n\n\nNeural Network Performance Summary:\n---------------------------------------------\nMetric          Training     Test        \n---------------------------------------------\nAccuracy        0.8634       0.7577      \nPrecision       0.7111       0.4071      \nRecall          0.5233       0.6032      \nF1-Score        0.6029       0.4861      \n\nTraining Information:\n---------------------------------------------\nFinal Loss:          0.306974\nEpochs Trained:      50000\nAvg Recent Loss:     0.306986\n\n\nThe accuracy dropped 2% but the F1 score increased by about 0.02. For our dataset, where the targets are not evenly distributed (80% of the wines are low quality), the F1 score provides a more meaningful evaluation metric. Unlike accuracy, which can be misinterpreted due to the majority class, F1 score balances precision and recall, offering a better reflection of model performance on both classes.\n\n\n\nModel Comparison\nOne final analysis that I’m interested in is to compare the predictions of the neural network and logistic regression models. Let’s see how well they agree on the test set.\n\n\nCode\n# Get model predictions and true labels\nnn_test_proba = results_nn['y_test_proba'].flatten()\nlr_test_proba = results_logistic['y_test_proba'].flatten()\n\nquality_counts = np.bincount(y_test_binary.flatten())\nnum_zeros = quality_counts[0]\nnum_ones = quality_counts[1]\n\nprint(f\"Number of poor quality wines: {num_zeros} - {num_zeros/len(y_test_binary)*100}%\")\nprint(f\"Number of good quality wines: {num_ones} - {num_ones/len(y_test_binary)*100}%\")\n\n\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\n\nlabels = y_test_binary.flatten()\n\n\ncmap = ListedColormap(['#1f77b4', '#d62728'])  # blue, red\nbounds = [-0.5, 0.5, 1.5]  # so that 0 maps to blue, 1 to red\nnorm = BoundaryNorm(bounds, cmap.N)\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(\n    lr_test_proba,\n    nn_test_proba,\n    c=labels,\n    cmap=cmap,\n    norm=norm,\n    s=40,\n    alpha=0.7,\n    edgecolors='k',\n    linewidths=0.5\n)\n\nplt.plot([0, 1], [0, 1], 'k--', label='Perfect Agreement')\n\nplt.title(\"Neural Network vs Logistic Regression Predictions\", fontsize=16, fontweight='bold')\nplt.xlabel(\"Logistic Regression Probability\", fontsize=14)\nplt.ylabel(\"Neural Network Probability\", fontsize=14)\nplt.grid(True, linestyle='--', alpha=0.4)\n\ncbar = plt.colorbar(scatter, ticks=[0, 1])\ncbar.ax.set_yticklabels(['Low Quality', 'High Quality'])\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nNumber of poor quality wines: 1053 - 81.0%\nNumber of good quality wines: 247 - 19.0%\n\n\n\n\n\n\n\n\n\nIt appears that the neural network and logistic regression models agree more on the high quality wines as indicated by the cluster of data in the top right corner of the scatter plot. We can also note that the neural network is more conservative in its predictions since nearly all the points are below the diagonal. This is in alignment with the logistic regression model’s recall (85%) and precision (24%) - suggesting that it frequently predicts many false positives."
  },
  {
    "objectID": "posts/wine_quality_prediction/index.html#conclusion-what-weve-learned",
    "href": "posts/wine_quality_prediction/index.html#conclusion-what-weve-learned",
    "title": "Predicting Wine Quality",
    "section": "Conclusion: What We’ve Learned",
    "text": "Conclusion: What We’ve Learned\nAs we wrap up this exploration, I’m struck by how much ground we’ve covered — not just in understanding and predicting wine quality, but in learning the fundamentals of machine learning through this hands-on experience.\n\nModel Performance Summary\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLinear Regression\n~0%\n–\n–\n–\n\n\nLogistic Regression\n46%\n0.24\n0.85\n0.38\n\n\nNeural Network\n77%\n0.41\n0.53\n0.46\n\n\n\n\n\nKey Learnings\nOn Wine Quality - There is a genuine relationship between wine quality and the chemical properties of the wine. - Alcohol content emerged as the strongest predictor of wine quality - higher alcohol wines tend to be rated more favorably. - Quality is not dictated by a single feature, but by the interplay between multiple chemical properties.\nOn Machine Learning - Incorporating all 11 chemical features significantly improved predictive performance — exactly what we’d expect in a multidimensional problem. - Neural networks require careful tuning. There are many parameters to consider (learning rate, number of epochs, the architecture of the network, etc.). Building these models requires careful thought and iteration. - Evaluating the performance of the model is not as straightforward as it seems. How accurately the model predicts the target variable in the test set is not enough. Other metrics must be considered to gain a fuller picture.\n\n\nNext Steps and Future Explorations\nThroughout this project, specifically while coding the neural network, there were several things I was curious to try. Specifically:\n\nChanging the model architecture - More layers, different activation functions, different neuron counts, etc.\nPlaying with the learning rate - specifically using algorithms such as Adam.\nExploring how stochastic gradient descent (SGD) compares to full batch gradient descent both in resultant model quality and training time.\n\nThanks for reading!"
  },
  {
    "objectID": "posts/ai-july-00/index.html",
    "href": "posts/ai-july-00/index.html",
    "title": "AI July - 00",
    "section": "",
    "text": "I’ve been building LLM-based apps for a while now, but recently I’ve been wanting to dig deeper - past the APIs and into the nuts and bolts of how these models actually work. So I’m carving out a month (AI July) to dive headfirst into deep learning. Here’s the plan:"
  },
  {
    "objectID": "posts/ai-july-00/index.html#week-1",
    "href": "posts/ai-july-00/index.html#week-1",
    "title": "AI July - 00",
    "section": "Week 1:",
    "text": "Week 1:\n\nRead chapters 1-5 of Dive into Deep Learning\nTrain NN from scratch on MNIST w/ PyTorch\nSupplemental lectures from Introduction to Deep Learning"
  },
  {
    "objectID": "posts/ai-july-00/index.html#week-2",
    "href": "posts/ai-july-00/index.html#week-2",
    "title": "AI July - 00",
    "section": "Week 2:",
    "text": "Week 2:\n\nFind a recent, interesting paper\nReproduce the results"
  },
  {
    "objectID": "posts/ai-july-00/index.html#week-3",
    "href": "posts/ai-july-00/index.html#week-3",
    "title": "AI July - 00",
    "section": "Week 3:",
    "text": "Week 3:\n\nExplore an active research area\nComplete a mini-study on the topic"
  },
  {
    "objectID": "posts/ai-july-00/index.html#week-4",
    "href": "posts/ai-july-00/index.html#week-4",
    "title": "AI July - 00",
    "section": "Week 4:",
    "text": "Week 4:\n\nCompile results and share findings\n\nA bit ambitious, but I’m excited to get started! I’ll be posting updates here as I go at least weekly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Predicting Wine Quality\n\n\n\n\n\n\n\n\nJul 21, 2025\n\n\nNick Mesmer\n\n\n\n\n\n\n\n\n\n\n\n\nAI July - 00\n\n\n\nai_july\n\n\n\n\n\n\n\n\n\nJul 1, 2025\n\n\nNick Mesmer\n\n\n\n\n\nNo matching items"
  }
]