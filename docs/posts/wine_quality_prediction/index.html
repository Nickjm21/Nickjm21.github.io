<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nick Mesmer">
<meta name="dcterms.date" content="2025-07-21">

<title>Predicting Wine Quality – Nick Mesmer Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Nick Mesmer Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/nicholas-mesmer"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Predicting Wine Quality</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Nick Mesmer </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 21, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-well-explore" id="toc-what-well-explore" class="nav-link active" data-scroll-target="#what-well-explore">What We’ll Explore</a></li>
  <li><a href="#data-exploration" id="toc-data-exploration" class="nav-link" data-scroll-target="#data-exploration">Data Exploration</a>
  <ul class="collapse">
  <li><a href="#feature-descriptions" id="toc-feature-descriptions" class="nav-link" data-scroll-target="#feature-descriptions">Feature Descriptions</a></li>
  </ul></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a>
  <ul class="collapse">
  <li><a href="#the-data-quality-check" id="toc-the-data-quality-check" class="nav-link" data-scroll-target="#the-data-quality-check">The Data Quality Check</a></li>
  <li><a href="#data-standardization" id="toc-data-standardization" class="nav-link" data-scroll-target="#data-standardization">Data Standardization</a></li>
  </ul></li>
  <li><a href="#machine-learning-models" id="toc-machine-learning-models" class="nav-link" data-scroll-target="#machine-learning-models">Machine Learning Models</a>
  <ul class="collapse">
  <li><a href="#simple-linear-regression-single-feature" id="toc-simple-linear-regression-single-feature" class="nav-link" data-scroll-target="#simple-linear-regression-single-feature">1. Simple Linear Regression (Single Feature)</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">2. Logistic Regression</a></li>
  <li><a href="#neural-network" id="toc-neural-network" class="nav-link" data-scroll-target="#neural-network">3. Neural Network</a></li>
  <li><a href="#model-comparison" id="toc-model-comparison" class="nav-link" data-scroll-target="#model-comparison">Model Comparison</a></li>
  </ul></li>
  <li><a href="#conclusion-what-weve-learned" id="toc-conclusion-what-weve-learned" class="nav-link" data-scroll-target="#conclusion-what-weve-learned">Conclusion: What We’ve Learned</a>
  <ul class="collapse">
  <li><a href="#model-performance-summary" id="toc-model-performance-summary" class="nav-link" data-scroll-target="#model-performance-summary">Model Performance Summary</a></li>
  <li><a href="#key-learnings" id="toc-key-learnings" class="nav-link" data-scroll-target="#key-learnings">Key Learnings</a></li>
  <li><a href="#next-steps-and-future-explorations" id="toc-next-steps-and-future-explorations" class="nav-link" data-scroll-target="#next-steps-and-future-explorations">Next Steps and Future Explorations</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<div class="subscription-widget">
  <h4 class="subscription-title">Stay Updated</h4>
  <p class="subscription-description">Get notified when I publish new posts!</p>
  
  <!-- Begin ButtonDown Signup Form -->
  <form action="https://buttondown.com/api/emails/embed-subscribe/NickMesmer" method="post" target="popupwindow" onsubmit="window.open('https://buttondown.com/NickMesmer', 'popupwindow')" class="embeddable-buttondown-form subscription-form">
    <div class="form-group">
      <input type="email" name="email" id="bd-email" placeholder="Enter your email" required="">
      <button type="submit" class="subscribe-btn">Subscribe</button>
    </div>
  </form>
  <!--End mc_embed_signup-->
</div> 
</div></div>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><em>Can we predict wine quality from its chemical properties alone?</em></p>
<p>In my day job, I develop AI tools to boost productivity and efficiency across my organization. While this occasionally involves designing machine learning architectures, much of it revolves around orchestrating LLM APIs like OpenAI’s or Anthropic’s.</p>
<p>Curious to deepen my understanding of how these models work under the hood, I’ve decided to start from the ground up—by implementing foundational learning algorithms from scratch.</p>
<p>Initially, I considered using the MNIST dataset to predict handwritten digits. However, I wanted something a bit more engaging and relatable. That’s when I came across this Wine Quality Dataset <span class="citation" data-cites="wine_quality_186">(<a href="#ref-wine_quality_186" role="doc-biblioref">Cortez and Reis 2009</a>)</span> from the UCI Machine Learning Repository.</p>
<p>This dataset contains several chemical properties of wine samples and their corresponding quality ratings (on a scale from 1 to 10). It immediately struck me as a more interesting and real-world challenge: predicting something as subjective as wine quality from raw chemical data.</p>
<section id="what-well-explore" class="level2">
<h2 class="anchored" data-anchor-id="what-well-explore">What We’ll Explore</h2>
<p>In this post, we’ll walk through:</p>
<ul>
<li><strong>The Dataset</strong>: Exploring 11 chemical features from 6,497 wine samples to uncover patterns beneath the surface.</li>
<li><strong>Machine Learning from Scratch</strong>: Implementing and comparing three models without using high-level ML libraries:
<ul>
<li>Simple Linear Regression (as a baseline)</li>
<li>Logistic Regression (to treat the problem as classification)</li>
<li>A Neural Network (to introduce deep learning fundamentals)</li>
</ul></li>
<li><strong>Practical Insights</strong>: Highlighting which chemical properties actually influence perceived wine quality.</li>
</ul>
<p>Let’s dive in and see what the data can reveal.</p>
<div id="b537a2a1" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import required libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np                    </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> time</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ucimlrepo <span class="im">import</span> fetch_ucirepo   <span class="co"># UCI Machine Learning Repository access</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility across runs</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure matplotlib for clean plots</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'default'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.facecolor'</span>] <span class="op">=</span> <span class="st">'white'</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Fetch the wine quality dataset from UCI ML Repository</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading Wine Quality dataset..."</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>wine_quality <span class="op">=</span> fetch_ucirepo(<span class="bu">id</span><span class="op">=</span><span class="dv">186</span>)  <span class="co"># ID 186 = Wine Quality dataset</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dataset loaded successfully!"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading Wine Quality dataset...
Dataset loaded successfully!</code></pre>
</div>
</div>
</section>
<section id="data-exploration" class="level2">
<h2 class="anchored" data-anchor-id="data-exploration">Data Exploration</h2>
<p>Let’s start by examining the structure and characteristics of our dataset. We’ll look at the features, their distributions, and understand what each variable represents in the context of wine quality assessment.</p>
<div id="6a4f5a2c" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract features and target variable</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> wine_quality.data.features </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> wine_quality.data.targets</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Display basic dataset information</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Dataset Information:"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of samples: </span><span class="sc">{</span><span class="bu">len</span>(X)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of features: </span><span class="sc">{</span><span class="bu">len</span>(X.columns)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample Data Preview:"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Feature Data (first 5 samples):"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>display(X.head())</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Target Data (first 5 samples):"</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>display(y.head())</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Target distribution (quality ratings):"</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>display(y[<span class="st">'quality'</span>].value_counts().sort_index())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset Information:
Number of samples: 6497
Number of features: 11


==================================================
Sample Data Preview:
==================================================

Feature Data (first 5 samples):</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">fixed_acidity</th>
<th data-quarto-table-cell-role="th">volatile_acidity</th>
<th data-quarto-table-cell-role="th">citric_acid</th>
<th data-quarto-table-cell-role="th">residual_sugar</th>
<th data-quarto-table-cell-role="th">chlorides</th>
<th data-quarto-table-cell-role="th">free_sulfur_dioxide</th>
<th data-quarto-table-cell-role="th">total_sulfur_dioxide</th>
<th data-quarto-table-cell-role="th">density</th>
<th data-quarto-table-cell-role="th">pH</th>
<th data-quarto-table-cell-role="th">sulphates</th>
<th data-quarto-table-cell-role="th">alcohol</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>7.4</td>
<td>0.70</td>
<td>0.00</td>
<td>1.9</td>
<td>0.076</td>
<td>11.0</td>
<td>34.0</td>
<td>0.9978</td>
<td>3.51</td>
<td>0.56</td>
<td>9.4</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>7.8</td>
<td>0.88</td>
<td>0.00</td>
<td>2.6</td>
<td>0.098</td>
<td>25.0</td>
<td>67.0</td>
<td>0.9968</td>
<td>3.20</td>
<td>0.68</td>
<td>9.8</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>7.8</td>
<td>0.76</td>
<td>0.04</td>
<td>2.3</td>
<td>0.092</td>
<td>15.0</td>
<td>54.0</td>
<td>0.9970</td>
<td>3.26</td>
<td>0.65</td>
<td>9.8</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>11.2</td>
<td>0.28</td>
<td>0.56</td>
<td>1.9</td>
<td>0.075</td>
<td>17.0</td>
<td>60.0</td>
<td>0.9980</td>
<td>3.16</td>
<td>0.58</td>
<td>9.8</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>7.4</td>
<td>0.70</td>
<td>0.00</td>
<td>1.9</td>
<td>0.076</td>
<td>11.0</td>
<td>34.0</td>
<td>0.9978</td>
<td>3.51</td>
<td>0.56</td>
<td>9.4</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Target Data (first 5 samples):</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">quality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>5</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>6</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Target distribution (quality ratings):</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>quality
3      30
4     216
5    2138
6    2836
7    1079
8     193
9       5
Name: count, dtype: int64</code></pre>
</div>
</div>
<section id="feature-descriptions" class="level3">
<h3 class="anchored" data-anchor-id="feature-descriptions">Feature Descriptions</h3>
<p>Understanding the physicochemical properties that influence wine quality is essential to our analysis. Below is a breakdown of each feature in the dataset:</p>
<section id="chemical-properties" class="level4">
<h4 class="anchored" data-anchor-id="chemical-properties">Chemical Properties</h4>
<ul>
<li><p><strong>Fixed Acidity</strong>: Tartaric acid content (g/dm³)<br>
→ The primary acid in grapes; contributes to a wine’s tartness and balances sweetness.</p></li>
<li><p><strong>Volatile Acidity</strong>: Acetic acid content (g/dm³)<br>
→ High levels can lead to a vinegar-like taste, often considered a flaw.</p></li>
<li><p><strong>Citric Acid</strong>: Citric acid content (g/dm³)<br>
→ Occasionally added to enhance freshness, flavor, and balance.</p></li>
<li><p><strong>pH</strong>: Acidity level (typically between 3 and 4)<br>
→ A lower pH indicates higher acidity, influencing taste and microbial stability.</p></li>
</ul>
</section>
<section id="preservation-additives" class="level4">
<h4 class="anchored" data-anchor-id="preservation-additives">Preservation &amp; Additives</h4>
<ul>
<li><p><strong>Free Sulfur Dioxide</strong>: Unbound SO₂ (mg/dm³)<br>
→ Acts as an antioxidant and antimicrobial agent; crucial for wine preservation.</p></li>
<li><p><strong>Total Sulfur Dioxide</strong>: Combined free and bound SO₂ (mg/dm³)<br>
→ Total measure of sulfur dioxide used in production and preservation.</p></li>
<li><p><strong>Sulphates</strong>: Sulfate content (g/dm³)<br>
→ A natural preservative that can enhance shelf life and protect against spoilage.</p></li>
<li><p><strong>Chlorides</strong>: Chloride content (g/dm³)<br>
→ Influenced by mineral content and location; excessive levels may lead to off-flavors.</p></li>
</ul>
</section>
<section id="structural-components" class="level4">
<h4 class="anchored" data-anchor-id="structural-components">Structural Components</h4>
<ul>
<li><p><strong>Density</strong>: Wine density (g/cm³)<br>
→ Correlated with residual sugar and alcohol content; can indicate fermentation progress.</p></li>
<li><p><strong>Residual Sugar</strong>: Unfermented grape sugars (g/dm³)<br>
→ Contributes to the wine’s sweetness and mouthfeel.</p></li>
<li><p><strong>Alcohol</strong>: Alcohol percentage by volume<br>
→ A primary driver of wine character and body; often correlates with quality scores.</p></li>
</ul>
<hr>
<p>Now that we understand what each feature represents, let’s visualize their distributions to get a better sense of how they vary across our dataset.</p>
<div id="8a06f8d0" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create comprehensive feature distribution plots</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">15</span>))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Wine Feature Distributions'</span>, fontsize<span class="op">=</span><span class="dv">18</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, y<span class="op">=</span><span class="fl">0.98</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define units for each feature</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>feature_units <span class="op">=</span> {</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'fixed_acidity'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">g/dm³</span><span class="kw">)</span><span class="vs">'</span>,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'volatile_acidity'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">g/dm³</span><span class="kw">)</span><span class="vs">'</span>,</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'citric_acid'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">g/dm³</span><span class="kw">)</span><span class="vs">'</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'residual_sugar'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">g/dm³</span><span class="kw">)</span><span class="vs">'</span>,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'chlorides'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">g/dm³</span><span class="kw">)</span><span class="vs">'</span>,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'free_sulfur_dioxide'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">mg/dm³</span><span class="kw">)</span><span class="vs">'</span>,</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'total_sulfur_dioxide'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">mg/dm³</span><span class="kw">)</span><span class="vs">'</span>,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'density'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">g/cm³</span><span class="kw">)</span><span class="vs">'</span>,</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pH'</span>: <span class="st">''</span>,</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'sulphates'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">g/dm³</span><span class="kw">)</span><span class="vs">'</span>,</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'alcohol'</span>: <span class="vs">r'</span><span class="kw">(</span><span class="vs">% vol</span><span class="dv">.</span><span class="kw">)</span><span class="vs">'</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten axes for easier indexing</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Create histograms for each feature</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, feature <span class="kw">in</span> <span class="bu">enumerate</span>(X.columns):</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[i]</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create histogram with improved styling</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    n, bins, patches <span class="op">=</span> ax.hist(X[feature], bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, </span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>                              color<span class="op">=</span><span class="st">'steelblue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate statistics</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    mean_val <span class="op">=</span> X[feature].mean()</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    std_val <span class="op">=</span> X[feature].std()</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add vertical lines for median</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    ax.axvline(mean_val, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>               label<span class="op">=</span><span class="ss">f'Mean: </span><span class="sc">{</span>mean_val<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add statistical information as text</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    stats_text <span class="op">=</span> <span class="ss">f'μ = </span><span class="sc">{</span>mean_val<span class="sc">:.2f}</span><span class="ch">\n</span><span class="ss">σ = </span><span class="sc">{</span>std_val<span class="sc">:.2f}</span><span class="ch">\n</span><span class="ss">Range: [</span><span class="sc">{</span>X[feature]<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.2f}</span><span class="ss">, </span><span class="sc">{</span>X[feature]<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.2f}</span><span class="ss">]'</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    ax.text(<span class="fl">0.02</span>, <span class="fl">0.98</span>, stats_text, transform<span class="op">=</span>ax.transAxes, fontsize<span class="op">=</span><span class="dv">9</span>,</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>            verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.9</span>))</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Format axis labels</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    unit <span class="op">=</span> feature_units.get(feature, <span class="st">''</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    feature_name <span class="op">=</span> feature.replace(<span class="st">"_"</span>, <span class="st">" "</span>).title()</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>    xlabel <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>feature_name<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>unit<span class="sc">}</span><span class="ss">'</span> <span class="cf">if</span> unit <span class="cf">else</span> feature_name</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Customize the plot</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    ax.set_title(feature_name, fontweight<span class="op">=</span><span class="st">'bold'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(xlabel, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Frequency'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    ax.legend(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Hide the unused subplot (12th position)</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">11</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust layout and display</span></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(top<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary statistics</span></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Feature Summary Statistics:"</span>)</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">60</span>)</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>display(X.describe().<span class="bu">round</span>(<span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Feature Summary Statistics:
============================================================</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">fixed_acidity</th>
<th data-quarto-table-cell-role="th">volatile_acidity</th>
<th data-quarto-table-cell-role="th">citric_acid</th>
<th data-quarto-table-cell-role="th">residual_sugar</th>
<th data-quarto-table-cell-role="th">chlorides</th>
<th data-quarto-table-cell-role="th">free_sulfur_dioxide</th>
<th data-quarto-table-cell-role="th">total_sulfur_dioxide</th>
<th data-quarto-table-cell-role="th">density</th>
<th data-quarto-table-cell-role="th">pH</th>
<th data-quarto-table-cell-role="th">sulphates</th>
<th data-quarto-table-cell-role="th">alcohol</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>6497.00</td>
<td>6497.00</td>
<td>6497.00</td>
<td>6497.00</td>
<td>6497.00</td>
<td>6497.00</td>
<td>6497.00</td>
<td>6497.00</td>
<td>6497.00</td>
<td>6497.00</td>
<td>6497.00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>7.22</td>
<td>0.34</td>
<td>0.32</td>
<td>5.44</td>
<td>0.06</td>
<td>30.53</td>
<td>115.74</td>
<td>0.99</td>
<td>3.22</td>
<td>0.53</td>
<td>10.49</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>1.30</td>
<td>0.16</td>
<td>0.15</td>
<td>4.76</td>
<td>0.04</td>
<td>17.75</td>
<td>56.52</td>
<td>0.00</td>
<td>0.16</td>
<td>0.15</td>
<td>1.19</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>3.80</td>
<td>0.08</td>
<td>0.00</td>
<td>0.60</td>
<td>0.01</td>
<td>1.00</td>
<td>6.00</td>
<td>0.99</td>
<td>2.72</td>
<td>0.22</td>
<td>8.00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>6.40</td>
<td>0.23</td>
<td>0.25</td>
<td>1.80</td>
<td>0.04</td>
<td>17.00</td>
<td>77.00</td>
<td>0.99</td>
<td>3.11</td>
<td>0.43</td>
<td>9.50</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>7.00</td>
<td>0.29</td>
<td>0.31</td>
<td>3.00</td>
<td>0.05</td>
<td>29.00</td>
<td>118.00</td>
<td>0.99</td>
<td>3.21</td>
<td>0.51</td>
<td>10.30</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>7.70</td>
<td>0.40</td>
<td>0.39</td>
<td>8.10</td>
<td>0.06</td>
<td>41.00</td>
<td>156.00</td>
<td>1.00</td>
<td>3.32</td>
<td>0.60</td>
<td>11.30</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>15.90</td>
<td>1.58</td>
<td>1.66</td>
<td>65.80</td>
<td>0.61</td>
<td>289.00</td>
<td>440.00</td>
<td>1.04</td>
<td>4.01</td>
<td>2.00</td>
<td>14.90</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Aside: <em>I can’t believe I only just learned about the pandas .describe() method. Incredibly helpful.</em></p>
</section>
</section>
</section>
<section id="data-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="data-preprocessing">Data Preprocessing</h2>
<p>Before we can train any models, we need to prepare the dataset. This includes checking for missing values, standardizing our features, and splitting the data into training and testing sets.</p>
<section id="the-data-quality-check" class="level3">
<h3 class="anchored" data-anchor-id="the-data-quality-check">The Data Quality Check</h3>
<p>First things first — let’s confirm that our dataset is complete. Missing values can disrupt training and lead to misleading results, so it’s important to address them up front.</p>
<div id="795c1329" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for missing values in features</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>missing_features <span class="op">=</span> X.isnull().<span class="bu">sum</span>()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Missing values in features:"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(missing_features)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Total missing values: </span><span class="sc">{</span>missing_features<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for missing values in target</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>missing_target <span class="op">=</span> y.isnull().<span class="bu">sum</span>()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Missing values in target: </span><span class="sc">{</span>missing_target<span class="sc">.</span>values[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> missing_features.<span class="bu">sum</span>() <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> missing_target.<span class="bu">sum</span>() <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">✓ Excellent! No missing values found in the dataset."</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">⚠ Warning: Found </span><span class="sc">{</span>missing_features<span class="sc">.</span><span class="bu">sum</span>() <span class="op">+</span> missing_target<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> missing values that need to be addressed."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Missing values in features:
fixed_acidity           0
volatile_acidity        0
citric_acid             0
residual_sugar          0
chlorides               0
free_sulfur_dioxide     0
total_sulfur_dioxide    0
density                 0
pH                      0
sulphates               0
alcohol                 0
dtype: int64

Total missing values: 0

Missing values in target: 0

✓ Excellent! No missing values found in the dataset.</code></pre>
</div>
</div>
</section>
<section id="data-standardization" class="level3">
<h3 class="anchored" data-anchor-id="data-standardization">Data Standardization</h3>
<p>Excellent, our dataset is complete! The next step is to standardize our features so they’re on a common scale.</p>
<p><strong>Why do we do this?</strong><br>
Consider the difference between alcohol percentage (typically 8–15%) and total sulfur dioxide (which can exceed 300 mg/dm³). Without standardization, features with larger numeric ranges can disproportionately influence our models — even if they’re not more important.</p>
<p><strong>The Solution</strong><br>
We’ll use Z-score normalization, which transforms each feature to have a mean of 0 and a standard deviation of 1:</p>
<p><span class="math display">\[
z = \frac{x - \mu}{\sigma}
\]</span></p>
<p>This ensures that all features contribute equally when we train our models.</p>
<p><strong>Train-Test Split</strong>: We’ll also split our data into training (80%) and testing (20%) sets. This allows us to evaluate how well our models perform on unseen data.</p>
<div id="4cf9d1b9" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define train-test split ratio</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>TRAIN_SIZE <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>TEST_SIZE <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> TRAIN_SIZE</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data split: </span><span class="sc">{</span>TRAIN_SIZE<span class="op">*</span><span class="dv">100</span><span class="sc">:.0f}</span><span class="ss">% training, </span><span class="sc">{</span>TEST_SIZE<span class="op">*</span><span class="dv">100</span><span class="sc">:.0f}</span><span class="ss">% testing"</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate standardization parameters from the full dataset</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>X_means <span class="op">=</span> X.mean()</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>X_stds <span class="op">=</span> X.std()</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>y_means <span class="op">=</span> y.mean()</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>y_stds <span class="op">=</span> y.std()</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply z-score standardization: (x - mean) / std</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># This transforms features to have mean=0 and std=1</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>X_norm <span class="op">=</span> (X <span class="op">-</span> X_means) <span class="op">/</span> X_stds</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>y_norm <span class="op">=</span> (y <span class="op">-</span> y_means) <span class="op">/</span> y_stds</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Standardization completed:"</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Features - Mean: </span><span class="sc">{</span>X_norm<span class="sc">.</span>mean()<span class="sc">.</span>mean()<span class="sc">:.2e}</span><span class="ss">, Std: </span><span class="sc">{</span>X_norm<span class="sc">.</span>std()<span class="sc">.</span>mean()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Target - Mean: </span><span class="sc">{</span>y_norm<span class="sc">.</span>mean()<span class="sc">.</span>values[<span class="dv">0</span>]<span class="sc">:.2e}</span><span class="ss">, Std: </span><span class="sc">{</span>y_norm<span class="sc">.</span>std()<span class="sc">.</span>values[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into training and testing sets</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>split_idx <span class="op">=</span> <span class="bu">int</span>(TRAIN_SIZE <span class="op">*</span> <span class="bu">len</span>(X_norm))</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_norm.iloc[:split_idx]</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_norm.iloc[split_idx:]</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_norm.iloc[:split_idx]</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y_norm.iloc[split_idx:]</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure no samples are lost in the split</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>data_integrity <span class="op">=</span> (<span class="bu">len</span>(X_norm) <span class="op">==</span> <span class="bu">len</span>(X_train) <span class="op">+</span> <span class="bu">len</span>(X_test)) <span class="kw">and</span> (<span class="bu">len</span>(y_norm) <span class="op">==</span> <span class="bu">len</span>(y_train) <span class="op">+</span> <span class="bu">len</span>(y_test))</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">No data lost: </span><span class="sc">{</span>data_integrity<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Display dataset information in a clean format</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Dataset Shapes:"</span>)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Set'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Features'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Target'</span><span class="sc">:&lt;15}</span><span class="ss">"</span>)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Training'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">str</span>(X_train.shape)<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">str</span>(y_train.shape)<span class="sc">:&lt;15}</span><span class="ss">"</span>)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Testing'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">str</span>(X_test.shape)<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">str</span>(y_test.shape)<span class="sc">:&lt;15}</span><span class="ss">"</span>)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Total'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">str</span>(X_norm.shape)<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">str</span>(y_norm.shape)<span class="sc">:&lt;15}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data split: 80% training, 20% testing

Standardization completed:
Features - Mean: -9.23e-17, Std: 1.00
Target - Mean: -2.89e-16, Std: 1.00

No data lost: True

Dataset Shapes:
Set        Features        Target         
----------------------------------------
Training   (5197, 11)      (5197, 1)      
Testing    (1300, 11)      (1300, 1)      
Total      (6497, 11)      (6497, 1)      </code></pre>
</div>
</div>
</section>
</section>
<section id="machine-learning-models" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-models">Machine Learning Models</h2>
<p>Now for the fun part — building models to predict wine quality! I’ll implement three approaches, each increasing in complexity. By building them from scratch, we’ll gain an intuitive understanding of what’s happening under the hood.</p>
<section id="simple-linear-regression-single-feature" class="level3">
<h3 class="anchored" data-anchor-id="simple-linear-regression-single-feature">1. Simple Linear Regression (Single Feature)</h3>
<p>Let’s begin with the most basic model: predicting wine quality using just one feature, pH. This serves as a baseline and helps us explore whether a simple linear relationship exists between acidity and quality.</p>
<p><strong>Why pH?</strong><br>
From our earlier exploration, pH was one of the most normally distributed features and represents a core characteristic of wine. Starting with a single feature also simplifies the math and makes model behavior easier to interpret.</p>
<section id="the-mathematical-model" class="level4">
<h4 class="anchored" data-anchor-id="the-mathematical-model">The Mathematical Model</h4>
<p>Our linear regression model is defined as:</p>
<p><span class="math display">\[
\hat{y} = \theta_0 + \theta_1 x
\]</span></p>
<p>Where <span class="math inline">\(\hat{y}\)</span> is the predicted wine quality (standardized), <span class="math inline">\(x\)</span> is the feature value (standardized pH value), and <span class="math inline">\(\theta_i\)</span> is the model parameter.</p>
</section>
<section id="cost-function" class="level4">
<h4 class="anchored" data-anchor-id="cost-function">Cost Function</h4>
<p>We’ll use Mean Squared Error (MSE) to measure model performance:</p>
<p><span class="math display">\[
J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2
\]</span></p>
<p>Where <span class="math inline">\(m\)</span> is the number of training samples, <span class="math inline">\(y^{(i)}\)</span> is the true value, and <span class="math inline">\(\hat{y}^{(i)}\)</span> is the predicted value.</p>
</section>
<section id="parameter-optimization" class="level4">
<h4 class="anchored" data-anchor-id="parameter-optimization">Parameter Optimization</h4>
<p>To minimize the cost, we’ll apply gradient descent. The gradients of the cost function with respect to each parameter are:</p>
<p><span class="math display">\[
\frac{\partial J}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})
\]</span></p>
<p><span class="math display">\[
\frac{\partial J}{\partial \theta_1} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)}) x^{(i)}
\]</span></p>
<p>We update the parameters using the learning rate <span class="math inline">\(\alpha\)</span>:</p>
<p><span class="math display">\[
\theta_j := \theta_j - \alpha \frac{\partial J}{\partial \theta_j}
\]</span></p>
<p>This iterative process continues until convergence — or until we’ve reached a maximum number of iterations.</p>
<hr>
<div id="80cbe7e9" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearRegression:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simple Linear Regression implementation using gradient descent</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">0.01</span>, max_iterations<span class="op">=</span><span class="dv">1000</span>, tolerance<span class="op">=</span><span class="fl">1e-10</span>):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialize the linear regression model</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">        - learning_rate: Step size for gradient descent</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">        - max_iterations: Maximum number of training iterations</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">        - tolerance: Convergence threshold (stop when cost change &lt; tolerance)</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_iterations <span class="op">=</span> max_iterations</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tolerance <span class="op">=</span> tolerance</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta_0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta_1 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cost_history <span class="op">=</span> []</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cost_function(<span class="va">self</span>, X, y):</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="co">        Calculate Mean Squared Error cost</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> <span class="va">self</span>.theta_0 <span class="op">+</span> <span class="va">self</span>.theta_1 <span class="op">*</span> X</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>((predictions <span class="op">-</span> y)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y):</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Train the model using gradient descent"""</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Record initial cost</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        initial_cost <span class="op">=</span> <span class="va">self</span>.cost_function(X, y)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cost_history.append(initial_cost)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Starting training with initial cost: </span><span class="sc">{</span>initial_cost<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.max_iterations):</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass: calculate predictions and errors</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> <span class="va">self</span>.theta_0 <span class="op">+</span> <span class="va">self</span>.theta_1 <span class="op">*</span> X</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>            errors <span class="op">=</span> predictions <span class="op">-</span> y</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate gradients (partial derivatives of cost function)</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>            theta_0_grad <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(errors)</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>            theta_1_grad <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(errors <span class="op">*</span> X)</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update parameters using gradient descent</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.theta_0 <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> theta_0_grad</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.theta_1 <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> theta_1_grad</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate new cost to track progress</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>            current_cost <span class="op">=</span> <span class="va">self</span>.cost_function(X, y)</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cost_history.append(current_cost)</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Print progress every 100 iterations</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> iteration <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span>iteration<span class="sc">:4d}</span><span class="ss">: Cost = </span><span class="sc">{</span>current_cost<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Check for convergence (stop if cost change is very small)</span></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.cost_history) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>                cost_change <span class="op">=</span> <span class="bu">abs</span>(<span class="va">self</span>.cost_history[<span class="op">-</span><span class="dv">2</span>] <span class="op">-</span> <span class="va">self</span>.cost_history[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> cost_change <span class="op">&lt;</span> <span class="va">self</span>.tolerance:</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"Converged at iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ss"> (cost change: </span><span class="sc">{</span>cost_change<span class="sc">:.2e}</span><span class="ss">)"</span>)</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>        final_cost <span class="op">=</span> <span class="va">self</span>.cost_history[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Training completed. Final cost: </span><span class="sc">{</span>final_cost<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a><span class="co">        Make predictions using the trained model</span></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.theta_0 <span class="op">+</span> <span class="va">self</span>.theta_1 <span class="op">*</span> X</span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data for simple linear regression</span></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>X_linear <span class="op">=</span> X_train[<span class="st">'pH'</span>].to_numpy()</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>y_linear <span class="op">=</span> y_train.to_numpy().flatten()</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Simple Linear Regression Training"</span>)</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Feature: pH"</span>)</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training samples: </span><span class="sc">{</span><span class="bu">len</span>(X_linear)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Learning rate: 0.01"</span>)</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a>model_linear <span class="op">=</span> LinearRegression(learning_rate<span class="op">=</span><span class="fl">0.1</span>, max_iterations<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>model_linear.fit(X_linear, y_linear)</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Learned parameters:"</span>)</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"θ_0 (intercept): </span><span class="sc">{</span>model_linear<span class="sc">.</span>theta_0<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"θ_1 (slope): </span><span class="sc">{</span>model_linear<span class="sc">.</span>theta_1<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Simple Linear Regression Training
========================================
Feature: pH
Training samples: 5197
Learning rate: 0.01

Starting training with initial cost: 0.519063
Iteration    0: Cost = 0.518987
Converged at iteration 70 (cost change: 8.29e-11)
Training completed. Final cost: 0.518638

Learned parameters:
θ_0 (intercept): -0.019141
θ_1 (slope): 0.023184</code></pre>
</div>
</div>
</section>
<section id="training-progress-visualization" class="level4">
<h4 class="anchored" data-anchor-id="training-progress-visualization">Training Progress Visualization</h4>
<p>Let’s visualize how the cost function decreased during training to understand the convergence behavior.</p>
<div id="abb8941d" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the cost history</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>cost_array <span class="op">=</span> model_linear.cost_history</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="bu">len</span>(cost_array)), cost_array, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">0.4</span>, marker<span class="op">=</span><span class="st">'o'</span>, markersize<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Cost Function History During Training'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iteration'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cost J(θ)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="bu">len</span>(cost_array)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Print some statistics about the cost reduction</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>initial_cost <span class="op">=</span> cost_array[<span class="dv">0</span>]</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>final_cost <span class="op">=</span> cost_array[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>cost_reduction <span class="op">=</span> initial_cost <span class="op">-</span> final_cost</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>percent_reduction <span class="op">=</span> (cost_reduction <span class="op">/</span> initial_cost) <span class="op">*</span> <span class="dv">100</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-evaluation-and-visualization" class="level4">
<h4 class="anchored" data-anchor-id="model-evaluation-and-visualization">Model Evaluation and Visualization</h4>
<p>Now let’s evaluate our simple linear regression model using standard metrics and visualize the results. We’ll calculate the <span class="math inline">\(R^2\)</span> score and create a comprehensive visualization of the model’s performance.</p>
<div id="bbffbc58" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the linear regression line with the data</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training data points</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_linear, y_linear, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">'lightblue'</span>, s<span class="op">=</span><span class="dv">30</span>, label<span class="op">=</span><span class="st">'Training Data'</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a range of x values for the regression line</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(X_linear.<span class="bu">min</span>(), X_linear.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model_linear.theta_0 <span class="op">+</span> model_linear.theta_1 <span class="op">*</span> x_range</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the regression line</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>plt.plot(x_range, y_pred, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="vs">fr'Regression Line: </span><span class="dv">$</span><span class="er">\</span><span class="vs">hat</span><span class="ch">{{</span><span class="vs">y</span><span class="ch">}}</span><span class="dv">$</span><span class="vs"> = </span><span class="sc">{</span>model_linear<span class="sc">.</span>theta_0<span class="sc">:.3f}</span><span class="vs"> </span><span class="op">+</span><span class="vs"> </span><span class="sc">{</span>model_linear<span class="sc">.</span>theta_1<span class="sc">:.3f}</span><span class="vs">x'</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Customize the plot</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Single Variable Linear Regression: pH vs Wine Quality'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'pH (standardized)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Wine Quality (standardized)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Add R^2 score</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>r_squared <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> (np.<span class="bu">sum</span>((y_linear <span class="op">-</span> (model_linear.theta_0 <span class="op">+</span> model_linear.theta_1 <span class="op">*</span> X_linear))<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> </span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>                 np.<span class="bu">sum</span>((y_linear <span class="op">-</span> np.mean(y_linear))<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>plt.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="ss">f'$R^2$ = </span><span class="sc">{</span>r_squared<span class="sc">:.4f}</span><span class="ss">'</span>, </span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>         transform<span class="op">=</span>plt.gca().transAxes, fontsize<span class="op">=</span><span class="dv">11</span>, </span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>         bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'white'</span>),</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>         verticalalignment<span class="op">=</span><span class="st">'top'</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>With an <span class="math inline">\(R^2\)</span> score of just 0.0005, our simple linear regression model is essentially useless for predicting wine quality. In other words, wine quality isn’t determined by pH alone. This intuitively makes sense.</p>
<p><strong>What Does This Tell Us?</strong> - The relationship between pH and wine quality is incredibly weak. - Wine quality likely depends on the interaction of many chemical factors. - We need a more sophisticated, multivariate approach.</p>
<p>This “failure” is actually valuable. It reinforces the complexity of the problem: wine quality isn’t something you can capture with a single variable. Let’s see how a more capable model performs.</p>
<hr>
</section>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">2. Logistic Regression</h3>
<p>Instead of predicting an exact score — which is subjective and often noisy — let’s simplify the problem:</p>
<blockquote class="blockquote">
<p>Is this wine high quality or not?</p>
</blockquote>
<p>We’ll define:</p>
<ul>
<li>High Quality: Wine quality <span class="math inline">\(&gt;= 7 \rightarrow\)</span> label = 1<br>
</li>
<li>Low Quality: Wine quality <span class="math inline">\(&lt; 7 \rightarrow\)</span> label = 0</li>
</ul>
<p>This framing allows us to use logistic regression, a foundational classification algorithm.</p>
<section id="the-mathematical-model-1" class="level4">
<h4 class="anchored" data-anchor-id="the-mathematical-model-1">The Mathematical Model</h4>
<p>Logistic regression applies the sigmoid function to the linear combination of inputs, transforming predictions into probabilities between 0 and 1:</p>
<p><span class="math display">\[
\hat{y} = \sigma(X\boldsymbol{\theta}) = \frac{1}{1 + e^{-X\boldsymbol{\theta}}}
\]</span></p>
<p>Where <span class="math inline">\(X\)</span> is our feature matrix (all 11 chemical properties), <span class="math inline">\(\boldsymbol{\theta}\)</span> is the parameter vector we’re learning, and <span class="math inline">\(\sigma\)</span> is the sigmoid activation function.</p>
</section>
<section id="parameter-optimization-1" class="level4">
<h4 class="anchored" data-anchor-id="parameter-optimization-1">Parameter Optimization</h4>
<p>To train this model, we’ll use Binary Cross-Entropy Loss:</p>
<p><span class="math display">\[
J(\boldsymbol{\theta}) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \right]
\]</span></p>
<p>What’s continually impressed me while learning about various machine learning architectures, is how simple the gradients often turn out to be. In this case, the gradient of the loss function is:</p>
<p><span class="math display">\[
\nabla J(\boldsymbol{\theta}) = \frac{1}{m} X^T (\hat{y} - y)
\]</span></p>
<p>This is almost identical to the gradient for linear regression — the only difference is that <span class="math inline">\(\hat{y}\)</span> now comes from the sigmoid function.</p>
<hr>
<div id="00d6e0d3" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Logistic Regression implementation using gradient descent</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">0.01</span>, max_iterations<span class="op">=</span><span class="dv">1000</span>, tolerance<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_iterations <span class="op">=</span> max_iterations</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tolerance <span class="op">=</span> tolerance</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cost_history <span class="op">=</span> []</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid(<span class="va">self</span>, z):</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sigmoid activation function"""</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> cost_function(<span class="va">self</span>, X, y):</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Calculate Binary Cross-Entropy cost"""</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> X <span class="op">@</span> <span class="va">self</span>.theta</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.sigmoid(z)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> <span class="op">-</span>(<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(y <span class="op">*</span> np.log(h) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> y) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> h))</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> cost</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y):</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Train the model using gradient descent"""</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        m, n <span class="op">=</span> X.shape</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize parameters</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.01</span>, (n, <span class="dv">1</span>))</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Record initial cost</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        initial_cost <span class="op">=</span> <span class="va">self</span>.cost_function(X, y)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cost_history.append(initial_cost)</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Starting logistic regression training..."</span>)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Initial cost: </span><span class="sc">{</span>initial_cost<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Features: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, Samples: </span><span class="sc">{</span>m<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.max_iterations):</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> X <span class="op">@</span> <span class="va">self</span>.theta</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> <span class="va">self</span>.sigmoid(z)</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate gradient</span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>            gradient <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X.T <span class="op">@</span> (h <span class="op">-</span> y)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update parameters</span></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.theta <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> gradient</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate new cost</span></span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>            current_cost <span class="op">=</span> <span class="va">self</span>.cost_function(X, y)</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cost_history.append(current_cost)</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Print progress</span></span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> iteration <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span>iteration<span class="sc">:4d}</span><span class="ss">: Cost = </span><span class="sc">{</span>current_cost<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Check for convergence</span></span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.cost_history) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>                cost_change <span class="op">=</span> <span class="bu">abs</span>(<span class="va">self</span>.cost_history[<span class="op">-</span><span class="dv">2</span>] <span class="op">-</span> <span class="va">self</span>.cost_history[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> cost_change <span class="op">&lt;</span> <span class="va">self</span>.tolerance:</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"Converged at iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ss"> (cost change: </span><span class="sc">{</span>cost_change<span class="sc">:.2e}</span><span class="ss">)"</span>)</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>        final_cost <span class="op">=</span> <span class="va">self</span>.cost_history[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Training completed. Final cost: </span><span class="sc">{</span>final_cost<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_proba(<span class="va">self</span>, X):</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Predict class probabilities"""</span></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> X <span class="op">@</span> <span class="va">self</span>.theta</span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.sigmoid(z)</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X, threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Make binary predictions"""</span></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>        probabilities <span class="op">=</span> <span class="va">self</span>.predict_proba(X)</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (probabilities <span class="op">&gt;=</span> threshold).astype(<span class="bu">int</span>)</span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preparation for logistic regression</span></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Preparing data for logistic regression..."</span>)</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform target to binary classification (quality &gt;= 7 = 1, else 0)</span></span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>quality_threshold <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>y_binary <span class="op">=</span> (y[<span class="st">'quality'</span>] <span class="op">&gt;=</span> quality_threshold).astype(<span class="bu">int</span>).values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Split binary target</span></span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>y_train_binary <span class="op">=</span> y_binary[:<span class="bu">len</span>(X_train)]</span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a>y_test_binary <span class="op">=</span> y_binary[<span class="bu">len</span>(X_train):]</span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a><span class="co"># Use standardized features</span></span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>X_train_logistic <span class="op">=</span> X_train.values</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>X_test_logistic <span class="op">=</span> X_test.values</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a><span class="co"># Display class distribution</span></span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a>train_positive <span class="op">=</span> np.<span class="bu">sum</span>(y_train_binary)</span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a>train_total <span class="op">=</span> <span class="bu">len</span>(y_train_binary)</span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a>test_positive <span class="op">=</span> np.<span class="bu">sum</span>(y_test_binary)</span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a>test_total <span class="op">=</span> <span class="bu">len</span>(y_test_binary)</span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Quality threshold: </span><span class="sc">{</span>quality_threshold<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set: </span><span class="sc">{</span>train_positive<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>train_total<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>train_positive<span class="op">/</span>train_total<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%) high quality"</span>)</span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test set: </span><span class="sc">{</span>test_positive<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>test_total<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>test_positive<span class="op">/</span>test_total<span class="op">*</span><span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%) high quality"</span>)</span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a><span class="co"># Train logistic regression model</span></span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a>model_logistic <span class="op">=</span> LogisticRegression(learning_rate<span class="op">=</span><span class="fl">0.2</span>, max_iterations<span class="op">=</span><span class="dv">250</span>)</span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a>model_logistic.fit(X_train_logistic, y_train_binary)</span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">45</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Preparing data for logistic regression...
=============================================
Quality threshold: 7
Training set: 1030/5197 (19.8%) high quality
Test set: 247/1300 (19.0%) high quality

Starting logistic regression training...
Initial cost: 0.695249
Features: 11, Samples: 5197

Iteration    0: Cost = 0.679790
Iteration  100: Cost = 0.588730
Iteration  200: Cost = 0.587354
Training completed. Final cost: 0.587092
=============================================</code></pre>
</div>
</div>
<div id="e1f412a8" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize training progress</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot cost history</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="bu">len</span>(model_logistic.cost_history)), model_logistic.cost_history, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Logistic Regression Training Progress'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cost (Binary Cross-Entropy)'</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training completed in </span><span class="sc">{</span><span class="bu">len</span>(model_logistic.cost_history)<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations"</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final training cost: </span><span class="sc">{</span>model_logistic<span class="sc">.</span>cost_history[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Training completed in 250 iterations
Final training cost: 0.587092</code></pre>
</div>
</div>
</section>
<section id="model-evaluation" class="level4">
<h4 class="anchored" data-anchor-id="model-evaluation">Model Evaluation:</h4>
<p>Since this model is more robust than our single-feature linear regression, it’s worth taking a closer look at how well it performs. Instead of relying solely on accuracy, we’ll evaluate it using a set of standard classification metrics:</p>
<ul>
<li><p><strong>Accuracy</strong>:<br>
The proportion of correct predictions overall. This gives a general sense of model performance but can be misleading on imbalanced datasets.</p></li>
<li><p><strong>Precision</strong>:<br>
The percentage of predicted <em>high-quality</em> wines that are actually high quality. High precision means the model makes few false positives.</p></li>
<li><p><strong>Recall</strong>:<br>
The percentage of <em>actual</em> high-quality wines that were correctly identified. High recall means the model catches most of the positives.</p></li>
<li><p><strong>F1 Score</strong>:<br>
The harmonic mean of precision and recall. It balances the trade-off between the two, especially useful when class distributions are uneven.</p></li>
</ul>
<p>These metrics help us evaluate how well the model performs across different dimensions of classification quality — not just how often it gets things right, but <em>what kind of mistakes it makes</em>.</p>
<div id="8f83adf0" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_model(model, X_train, y_train, X_test, y_test, loss_history<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Evaluate a classification or neural network model and return metrics.</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - model: A trained model with predict() and predict_proba() methods.</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">    - X_train, y_train: Training data and labels.</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - X_test, y_test: Testing data and labels.</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - loss_history (optional): List of loss values for neural networks.</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co">    - dict of evaluation metrics.</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make predictions</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    y_train_pred <span class="op">=</span> model.predict(X_train)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    y_test_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    y_train_proba <span class="op">=</span> model.predict_proba(X_train)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    y_test_proba <span class="op">=</span> model.predict_proba(X_test)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Metrics</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> accuracy(y_true, y_pred):</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean(y_true.flatten() <span class="op">==</span> y_pred.flatten())</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> precision(y_true, y_pred):</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        tp <span class="op">=</span> np.<span class="bu">sum</span>((y_true <span class="op">==</span> <span class="dv">1</span>) <span class="op">&amp;</span> (y_pred <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        fp <span class="op">=</span> np.<span class="bu">sum</span>((y_true <span class="op">==</span> <span class="dv">0</span>) <span class="op">&amp;</span> (y_pred <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tp <span class="op">/</span> (tp <span class="op">+</span> fp) <span class="cf">if</span> (tp <span class="op">+</span> fp) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> recall(y_true, y_pred):</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>        tp <span class="op">=</span> np.<span class="bu">sum</span>((y_true <span class="op">==</span> <span class="dv">1</span>) <span class="op">&amp;</span> (y_pred <span class="op">==</span> <span class="dv">1</span>))</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>        fn <span class="op">=</span> np.<span class="bu">sum</span>((y_true <span class="op">==</span> <span class="dv">1</span>) <span class="op">&amp;</span> (y_pred <span class="op">==</span> <span class="dv">0</span>))</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tp <span class="op">/</span> (tp <span class="op">+</span> fn) <span class="cf">if</span> (tp <span class="op">+</span> fn) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> f1_score(prec, rec):</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">2</span> <span class="op">*</span> (prec <span class="op">*</span> rec) <span class="op">/</span> (prec <span class="op">+</span> rec) <span class="cf">if</span> (prec <span class="op">+</span> rec) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute metrics</span></span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    train_acc <span class="op">=</span> accuracy(y_train, y_train_pred)</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>    test_acc <span class="op">=</span> accuracy(y_test, y_test_pred)</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    train_prec <span class="op">=</span> precision(y_train, y_train_pred)</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>    test_prec <span class="op">=</span> precision(y_test, y_test_pred)</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>    train_rec <span class="op">=</span> recall(y_train, y_train_pred)</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>    test_rec <span class="op">=</span> recall(y_test, y_test_pred)</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>    train_f1 <span class="op">=</span> f1_score(train_prec, train_rec)</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>    test_f1 <span class="op">=</span> f1_score(test_prec, test_rec)</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> {</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>        <span class="st">'train_accuracy'</span>: train_acc,</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>        <span class="st">'test_accuracy'</span>: test_acc,</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a>        <span class="st">'train_precision'</span>: train_prec,</span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>        <span class="st">'test_precision'</span>: test_prec,</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>        <span class="st">'train_recall'</span>: train_rec,</span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>        <span class="st">'test_recall'</span>: test_rec,</span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>        <span class="st">'train_f1'</span>: train_f1,</span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>        <span class="st">'test_f1'</span>: test_f1,</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a>        <span class="st">'y_train_pred'</span>: y_train_pred,</span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a>        <span class="st">'y_test_pred'</span>: y_test_pred,</span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a>        <span class="st">'y_train_proba'</span>: y_train_proba,</span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>        <span class="st">'y_test_proba'</span>: y_test_proba</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Added after coding the neural network</span></span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> loss_history <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a>        recent_loss <span class="op">=</span> np.mean(loss_history[<span class="op">-</span><span class="dv">100</span>:]) <span class="cf">if</span> <span class="bu">len</span>(loss_history) <span class="op">&gt;=</span> <span class="dv">100</span> <span class="cf">else</span> loss_history[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>        results.update({</span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a>            <span class="st">'final_loss'</span>: loss_history[<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a>            <span class="st">'avg_recent_loss'</span>: recent_loss,</span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a>            <span class="st">'epochs_trained'</span>: <span class="bu">len</span>(loss_history),</span>
<span id="cb22-72"><a href="#cb22-72" aria-hidden="true" tabindex="-1"></a>            <span class="st">'loss_history'</span>: loss_history</span>
<span id="cb22-73"><a href="#cb22-73" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb22-74"><a href="#cb22-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-75"><a href="#cb22-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span>
<span id="cb22-76"><a href="#cb22-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-77"><a href="#cb22-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-78"><a href="#cb22-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate logistic regression model</span></span>
<span id="cb22-79"><a href="#cb22-79" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Evaluating Logistic Regression Model..."</span>)</span>
<span id="cb22-80"><a href="#cb22-80" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb22-81"><a href="#cb22-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-82"><a href="#cb22-82" aria-hidden="true" tabindex="-1"></a>results_logistic <span class="op">=</span> evaluate_model(</span>
<span id="cb22-83"><a href="#cb22-83" aria-hidden="true" tabindex="-1"></a>    model_logistic, X_train_logistic, y_train_binary, X_test_logistic, y_test_binary</span>
<span id="cb22-84"><a href="#cb22-84" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-85"><a href="#cb22-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-86"><a href="#cb22-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classification Performance Summary:"</span>)</span>
<span id="cb22-87"><a href="#cb22-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb22-88"><a href="#cb22-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Metric'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Training'</span><span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Test'</span><span class="sc">:&lt;12}</span><span class="ss">"</span>)</span>
<span id="cb22-89"><a href="#cb22-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb22-90"><a href="#cb22-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Accuracy'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_logistic[<span class="st">'train_accuracy'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_logistic[<span class="st">'test_accuracy'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-91"><a href="#cb22-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Precision'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_logistic[<span class="st">'train_precision'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_logistic[<span class="st">'test_precision'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-92"><a href="#cb22-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Recall'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_logistic[<span class="st">'train_recall'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_logistic[<span class="st">'test_recall'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-93"><a href="#cb22-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'F1-Score'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_logistic[<span class="st">'train_f1'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_logistic[<span class="st">'test_f1'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Evaluating Logistic Regression Model...
=============================================
Classification Performance Summary:
---------------------------------------------
Metric          Training     Test        
---------------------------------------------
Accuracy        0.7129       0.4623      
Precision       0.3900       0.2408      
Recall          0.7951       0.8502      
F1-Score        0.5233       0.3753      </code></pre>
</div>
</div>
</section>
<section id="results" class="level4">
<h4 class="anchored" data-anchor-id="results">Results</h4>
<p>Looking at the results, I’m honestly pleasantly surprised. While a test accuracy of <strong>46%</strong> may seem low at first glance, it’s a significant improvement over our linear regression baseline. To me, this suggests that there is <em>genuinely</em> a relationship between the chemical properties of the wines and their perceived quality.</p>
<p><strong>What’s Working Well</strong></p>
<ul>
<li>High Recall (85%): We’re successfully identifying most high-quality wines — and that’s important. In real-world terms, we’d rather flag too many good wines than miss the truly great ones.</li>
<li>Training vs.&nbsp;Test Performance: While there’s a performance gap, it’s not extreme. This indicates the model is generalizing reasonably well and not drastically overfitting.</li>
</ul>
<p><strong>What Needs Work</strong></p>
<ul>
<li>Low Precision (24%): We’re making a lot of false positives — predicting wines are high quality when they’re not. This means the model is overconfident.</li>
<li>Moderate Accuracy: While better than chance, the overall accuracy leaves room for improvement. Clearly, the task is more complex than our current model can capture.</li>
</ul>
<p><strong>The Bottom Line</strong><br>
Our logistic regression model is overly optimistic, but it’s doing a decent job at identifying most truly great wines. That’s a promising result and it motivates us to go deeper.</p>
<p>Let’s investigate which chemical features are actually driving these predictions.</p>
<div id="f9673816" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature importance</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> X_train.columns</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>coefficients <span class="op">=</span> model_logistic.theta.flatten()</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>importance <span class="op">=</span> np.<span class="bu">abs</span>(coefficients)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>sorted_idx <span class="op">=</span> np.argsort(importance)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>plt.barh(<span class="bu">range</span>(<span class="bu">len</span>(feature_names)), importance[sorted_idx], color<span class="op">=</span><span class="st">'steelblue'</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>plt.yticks(<span class="bu">range</span>(<span class="bu">len</span>(feature_names)), [feature_names[i] <span class="cf">for</span> i <span class="kw">in</span> sorted_idx])</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feature Importance (|Coefficient|)'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Absolute Coefficient Value'</span>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Print top influential features</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 5 Most Influential Features:"</span>)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">40</span>)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(sorted_idx[:<span class="dv">5</span>]):</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    feature <span class="op">=</span> feature_names[idx]</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    coef <span class="op">=</span> coefficients[idx]</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>feature<span class="sc">:&lt;20}</span><span class="ss"> (coef: </span><span class="sc">{</span>coef<span class="sc">:+.4f}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 5 Most Influential Features:
========================================
1. alcohol              (coef: +0.7083)
2. residual_sugar       (coef: +0.4670)
3. density              (coef: -0.4646)
4. sulphates            (coef: +0.2703)
5. total_sulfur_dioxide (coef: -0.2338)</code></pre>
</div>
</div>
</section>
<section id="interpreting-feature-importance" class="level4">
<h4 class="anchored" data-anchor-id="interpreting-feature-importance">Interpreting Feature Importance</h4>
<p>From the plot above, it’s clear that alcohol is by far the most influential feature in predicting whether a wine is classified as high quality. This aligns with intuition — alcohol content often correlates with flavor intensity and mouthfeel, which may contribute to higher quality scores.</p>
</section>
<section id="surprises" class="level4">
<h4 class="anchored" data-anchor-id="surprises">Surprises</h4>
<p>After alcohol, residual sugar and density are the next most influential features. These, along with alcohol, are the three <em>structural</em> features of our dataset. It’s interesting that they play the largest roles in determining the overall quality of the wines.</p>
<p>Sugar and density are often linked — higher sugar content typically increases a wine’s density — and both contribute to its body and perceived richness.</p>
<p>However, the coefficients tell an interesting story:</p>
<ul>
<li><strong>Residual sugar</strong> has a <em>positive</em> coefficient: higher sugar levels increase the likelihood of being classified as high quality.<br>
</li>
<li><strong>Density</strong>, despite its correlation with sugar, has a <em>negative</em> coefficient: higher density reduces the likelihood.</li>
</ul>
<p><strong>What Could Explain This?</strong></p>
<p>Here are a few possible interpretations:</p>
<ul>
<li><p><strong>Alcohol vs.&nbsp;Sugar Tradeoff</strong>: Higher-density wines may also result from unfermented sugars — which can indicate a lower alcohol level. Since alcohol is strongly associated with higher quality in this model, density may be acting as a <strong>proxy for lower alcohol</strong>, hence the negative sign.</p></li>
<li><p><strong>Perception of Balance</strong>: Wines that are dense <em>but not sweet</em> may be perceived as heavy or cloying. In contrast, wines with balanced residual sugar and lower density may be seen as more elegant or refined.</p></li>
<li><p><strong>Nonlinearity</strong>: It’s possible that the relationship between density and wine quality isn’t truly linear. A logistic regression can’t capture this, so it may incorrectly assign a negative weight to a feature that actually has a more complex relationship with the outcome.</p></li>
</ul>
<p>This is a great example of how model interpretation isn’t always straightforward — and why exploring deeper models or visualizing feature interactions can uncover more nuanced relationships.</p>
</section>
<section id="a-quick-detour-density-vs.-alcohol" class="level4">
<h4 class="anchored" data-anchor-id="a-quick-detour-density-vs.-alcohol">A Quick Detour: Density vs.&nbsp;Alcohol</h4>
<p>I can’t help but further explore this — let’s visualize the relationship directly.</p>
<div id="d52e6435" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot density vs alcohol</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>density <span class="op">=</span> X[X[<span class="st">'density'</span>] <span class="op">&lt;</span> <span class="fl">1.01</span>][<span class="st">'density'</span>].values <span class="co"># There's two wines with oddly high densities. Removing to make the plot look nicer</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>alcohol <span class="op">=</span> X[X[<span class="st">'density'</span>] <span class="op">&lt;</span> <span class="fl">1.01</span>][<span class="st">'alcohol'</span>].values</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(x<span class="op">=</span>density, y<span class="op">=</span>alcohol, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Density vs Alcohol'</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Density'</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Alcohol'</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As we can see, there’s a clear inverse linear relationship between density and alcohol. In general, wines with higher density tend to have lower alcohol content.</p>
<p>This supports the idea that density is acting as a proxy for incomplete fermentation. During fermentation, sugar is converted into alcohol — so wines that are both dense and low in alcohol may simply be under-fermented or sweeter styles that the model associates with lower quality.</p>
<p>In contrast, alcohol is a strong positive predictor of quality in our model. So if the model sees high density <em>and</em> low alcohol, it logically adjusts the density coefficient to be negative in order to counteract the quality signal from residual sugar.</p>
<p>This kind of interaction — where two correlated features pull in opposite directions — is common in linear models and one reason why feature interpretation requires care. It’s not always about direct relationships, but about how the model distributes “credit” when multiple features overlap in what they explain.</p>
</section>
</section>
<section id="neural-network" class="level3">
<h3 class="anchored" data-anchor-id="neural-network">3. Neural Network</h3>
<p>Now for the most sophisticated model — let’s build a neural network. This is where things get exciting (and a bit more complex). Neural networks offer far more flexibility than linear models, giving them the power to learn intricate patterns that simpler models might miss.</p>
<section id="the-architecture" class="level4">
<h4 class="anchored" data-anchor-id="the-architecture">The Architecture</h4>
<p>To start, I’m keeping things intentionally simple:</p>
<ul>
<li><strong>Input Layer</strong>: 11 neurons (one for each standardized chemical feature)</li>
<li><strong>Hidden Layer</strong>: 11 neurons
<ul>
<li>Mirroring the input size</li>
</ul></li>
<li><strong>Output Layer</strong>: 1 neuron
<ul>
<li>Outputs a probability between 0 and 1, representing the likelihood that a wine is high quality.</li>
</ul></li>
</ul>
<p>This architecture gives us just enough capacity to model interactions between features without overcomplicating things. Later, we can experiment with deeper networks or regularization techniques to fine-tune performance.</p>
<div id="46407a97" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork:</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Single hidden layer neural network implementation</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, max_epochs<span class="op">=</span><span class="dv">5000</span>, tolerance<span class="op">=</span><span class="fl">0.00001</span>):</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_epochs <span class="op">=</span> max_epochs</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tolerance <span class="op">=</span> tolerance</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta_1 <span class="op">=</span> <span class="va">None</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta_2 <span class="op">=</span> <span class="va">None</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_history <span class="op">=</span> []</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.is_trained <span class="op">=</span> <span class="va">False</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _sigmoid(<span class="va">self</span>, z):</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sigmoid activation function"""</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _bce_loss(<span class="va">self</span>, y, y_hat, eps<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Binary cross-entropy loss"""</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> np.clip(y_hat, eps, <span class="dv">1</span> <span class="op">-</span> eps)</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>np.mean(y <span class="op">*</span> np.log(y_hat) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> y) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> y_hat))</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a><span class="co">        Train the neural network</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a><span class="co">        X : array-like, shape (m, n) - training features</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a><span class="co">        y : array-like, shape (m, 1) - training targets (binary)</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prepare data</span></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> X.copy()</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>        m, n <span class="op">=</span> x.shape</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.hstack([np.ones((m, <span class="dv">1</span>)), x])  <span class="co"># Add bias</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">#self.theta_1 = np.random.normal(0, 0.01, size=(n+1, n))</span></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#self.theta_2 = np.random.normal(0, 0.01, size=(n+1, 1))</span></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta_1 <span class="op">=</span> np.random.randn(n<span class="op">+</span><span class="dv">1</span>, n) <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">/</span> (n<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta_2 <span class="op">=</span> np.random.randn(n<span class="op">+</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">/</span> (n<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss_history <span class="op">=</span> []</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Starting Neural Network training..."</span>)</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Samples: </span><span class="sc">{</span>m<span class="sc">}</span><span class="ss">, Learning rate: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>learning_rate<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>()</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training loop</span></span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.max_epochs):</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>            z_2 <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.theta_1</span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>            a_2 <span class="op">=</span> <span class="va">self</span>._sigmoid(z_2)</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>            a_2 <span class="op">=</span> np.hstack([np.ones((m, <span class="dv">1</span>)), a_2])  <span class="co"># Add bias</span></span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>            z_3 <span class="op">=</span> a_2 <span class="op">@</span> <span class="va">self</span>.theta_2</span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>            a_3 <span class="op">=</span> <span class="va">self</span>._sigmoid(z_3)</span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate loss</span></span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">self</span>._bce_loss(y, a_3)</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.loss_history.append(loss)</span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Print progress</span></span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> verbose:</span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">:4d}</span><span class="ss">: Loss = </span><span class="sc">{</span>loss<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Check for convergence</span></span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.loss_history) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a>                loss_change <span class="op">=</span> <span class="bu">abs</span>(<span class="va">self</span>.loss_history[<span class="op">-</span><span class="dv">2</span>] <span class="op">-</span> <span class="va">self</span>.loss_history[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> loss_change <span class="op">&lt;</span> <span class="va">self</span>.tolerance:</span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"Converged at epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> (loss change: </span><span class="sc">{</span>loss_change<span class="sc">:.2e}</span><span class="ss">)"</span>)</span>
<span id="cb27-74"><a href="#cb27-74" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb27-75"><a href="#cb27-75" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb27-76"><a href="#cb27-76" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Backward pass - BCE loss with sigmoid</span></span>
<span id="cb27-77"><a href="#cb27-77" aria-hidden="true" tabindex="-1"></a>            delta_3 <span class="op">=</span> a_3 <span class="op">-</span> y</span>
<span id="cb27-78"><a href="#cb27-78" aria-hidden="true" tabindex="-1"></a>            grad_theta_2 <span class="op">=</span> (a_2.T <span class="op">@</span> delta_3) <span class="op">/</span> m</span>
<span id="cb27-79"><a href="#cb27-79" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb27-80"><a href="#cb27-80" aria-hidden="true" tabindex="-1"></a>            delta_2_full <span class="op">=</span> delta_3 <span class="op">@</span> <span class="va">self</span>.theta_2.T</span>
<span id="cb27-81"><a href="#cb27-81" aria-hidden="true" tabindex="-1"></a>            delta_2 <span class="op">=</span> delta_2_full[:, <span class="dv">1</span>:]</span>
<span id="cb27-82"><a href="#cb27-82" aria-hidden="true" tabindex="-1"></a>            sigmoid_grad <span class="op">=</span> a_2[:, <span class="dv">1</span>:] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> a_2[:, <span class="dv">1</span>:])</span>
<span id="cb27-83"><a href="#cb27-83" aria-hidden="true" tabindex="-1"></a>            delta_2 <span class="op">*=</span> sigmoid_grad  <span class="co"># element-wise</span></span>
<span id="cb27-84"><a href="#cb27-84" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb27-85"><a href="#cb27-85" aria-hidden="true" tabindex="-1"></a>            grad_theta_1 <span class="op">=</span> (x.T <span class="op">@</span> delta_2) <span class="op">/</span> m</span>
<span id="cb27-86"><a href="#cb27-86" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb27-87"><a href="#cb27-87" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Gradient descent</span></span>
<span id="cb27-88"><a href="#cb27-88" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.theta_1 <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> grad_theta_1</span>
<span id="cb27-89"><a href="#cb27-89" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.theta_2 <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> grad_theta_2</span>
<span id="cb27-90"><a href="#cb27-90" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-91"><a href="#cb27-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.is_trained <span class="op">=</span> <span class="va">True</span></span>
<span id="cb27-92"><a href="#cb27-92" aria-hidden="true" tabindex="-1"></a>        final_loss <span class="op">=</span> <span class="va">self</span>.loss_history[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb27-93"><a href="#cb27-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb27-94"><a href="#cb27-94" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Training completed. Final loss: </span><span class="sc">{</span>final_loss<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb27-95"><a href="#cb27-95" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"="</span> <span class="op">*</span> <span class="dv">50</span>)</span>
<span id="cb27-96"><a href="#cb27-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb27-97"><a href="#cb27-97" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-98"><a href="#cb27-98" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_proba(<span class="va">self</span>, X):</span>
<span id="cb27-99"><a href="#cb27-99" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb27-100"><a href="#cb27-100" aria-hidden="true" tabindex="-1"></a><span class="co">        Predict class probabilities</span></span>
<span id="cb27-101"><a href="#cb27-101" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb27-102"><a href="#cb27-102" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb27-103"><a href="#cb27-103" aria-hidden="true" tabindex="-1"></a><span class="co">        X : array-like, shape (m, n) - input features</span></span>
<span id="cb27-104"><a href="#cb27-104" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb27-105"><a href="#cb27-105" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb27-106"><a href="#cb27-106" aria-hidden="true" tabindex="-1"></a><span class="co">        probabilities : array-like, shape (m, 1) - predicted probabilities</span></span>
<span id="cb27-107"><a href="#cb27-107" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb27-108"><a href="#cb27-108" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.is_trained:</span>
<span id="cb27-109"><a href="#cb27-109" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Model must be trained before making predictions"</span>)</span>
<span id="cb27-110"><a href="#cb27-110" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-111"><a href="#cb27-111" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> X.copy()</span>
<span id="cb27-112"><a href="#cb27-112" aria-hidden="true" tabindex="-1"></a>        m, n <span class="op">=</span> x.shape</span>
<span id="cb27-113"><a href="#cb27-113" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.hstack([np.ones((m, <span class="dv">1</span>)), x])  <span class="co"># Add bias</span></span>
<span id="cb27-114"><a href="#cb27-114" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-115"><a href="#cb27-115" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb27-116"><a href="#cb27-116" aria-hidden="true" tabindex="-1"></a>        z_2 <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.theta_1</span>
<span id="cb27-117"><a href="#cb27-117" aria-hidden="true" tabindex="-1"></a>        a_2 <span class="op">=</span> <span class="va">self</span>._sigmoid(z_2)</span>
<span id="cb27-118"><a href="#cb27-118" aria-hidden="true" tabindex="-1"></a>        a_2 <span class="op">=</span> np.hstack([np.ones((m, <span class="dv">1</span>)), a_2])  <span class="co"># Add bias</span></span>
<span id="cb27-119"><a href="#cb27-119" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-120"><a href="#cb27-120" aria-hidden="true" tabindex="-1"></a>        z_3 <span class="op">=</span> a_2 <span class="op">@</span> <span class="va">self</span>.theta_2</span>
<span id="cb27-121"><a href="#cb27-121" aria-hidden="true" tabindex="-1"></a>        a_3 <span class="op">=</span> <span class="va">self</span>._sigmoid(z_3)</span>
<span id="cb27-122"><a href="#cb27-122" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-123"><a href="#cb27-123" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a_3</span>
<span id="cb27-124"><a href="#cb27-124" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-125"><a href="#cb27-125" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X, threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb27-126"><a href="#cb27-126" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb27-127"><a href="#cb27-127" aria-hidden="true" tabindex="-1"></a><span class="co">        Make binary predictions</span></span>
<span id="cb27-128"><a href="#cb27-128" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb27-129"><a href="#cb27-129" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb27-130"><a href="#cb27-130" aria-hidden="true" tabindex="-1"></a><span class="co">        X : array-like, shape (m, n) - input features</span></span>
<span id="cb27-131"><a href="#cb27-131" aria-hidden="true" tabindex="-1"></a><span class="co">        threshold : float - decision threshold (default: 0.5)</span></span>
<span id="cb27-132"><a href="#cb27-132" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb27-133"><a href="#cb27-133" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb27-134"><a href="#cb27-134" aria-hidden="true" tabindex="-1"></a><span class="co">        predictions : array-like, shape (m, 1) - binary predictions</span></span>
<span id="cb27-135"><a href="#cb27-135" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb27-136"><a href="#cb27-136" aria-hidden="true" tabindex="-1"></a>        probabilities <span class="op">=</span> <span class="va">self</span>.predict_proba(X)</span>
<span id="cb27-137"><a href="#cb27-137" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (probabilities <span class="op">&gt;=</span> threshold).astype(<span class="bu">int</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s run and evaluate it using the same function as before.</p>
<div id="dfdaa95f" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and train the neural network</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>model_nn <span class="op">=</span> NeuralNetwork(learning_rate<span class="op">=</span><span class="fl">0.1</span>, max_epochs<span class="op">=</span><span class="dv">5000</span>, tolerance<span class="op">=</span><span class="fl">1e-10</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>model_nn.fit(X_train_logistic, y_train_binary,verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the neural network model</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>results_nn <span class="op">=</span> evaluate_model(</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    model_nn, X_train_logistic, y_train_binary, X_test_logistic, y_test_binary, model_nn.loss_history</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Starting Neural Network training...
Samples: 5197, Learning rate: 0.1

Epoch    0: Loss = 0.775274
Epoch 1000: Loss = 0.382044
Epoch 2000: Loss = 0.373464
Epoch 3000: Loss = 0.370568
Epoch 4000: Loss = 0.368550
Training completed. Final loss: 0.366776
==================================================</code></pre>
</div>
</div>
<div id="26142936" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print statements to make the output look nice</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Neural Network Performance Summary:"</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Metric'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Training'</span><span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Test'</span><span class="sc">:&lt;12}</span><span class="ss">"</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Accuracy'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'train_accuracy'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'test_accuracy'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Precision'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'train_precision'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'test_precision'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Recall'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'train_recall'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'test_recall'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'F1-Score'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'train_f1'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'test_f1'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Training Information:"</span>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Final Loss:'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'final_loss'</span>]<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Epochs Trained:'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'epochs_trained'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Avg Recent Loss:'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'avg_recent_loss'</span>]<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Neural Network Performance Summary:
---------------------------------------------
Metric          Training     Test        
---------------------------------------------
Accuracy        0.8311       0.7831      
Precision       0.6203       0.4314      
Recall          0.3806       0.4453      
F1-Score        0.4717       0.4382      

Training Information:
---------------------------------------------
Final Loss:          0.366776
Epochs Trained:      5000
Avg Recent Loss:     0.366860</code></pre>
</div>
</div>
</section>
<section id="results-1" class="level4">
<h4 class="anchored" data-anchor-id="results-1">Results</h4>
<p>A 78% accuracy on our test set! That’s quite the improvement over logistic regression. Our precision, recall, and f1 score is still quite low but the NN is performing better than our logistic regression model as expected.</p>
</section>
<section id="surprises-1" class="level4">
<h4 class="anchored" data-anchor-id="surprises-1">Surprises</h4>
<p>One thing I’ve noticed while experimenting with this neural network is how sensitive the performance is to the number of epochs. This begs the question: How long should we train our model?</p>
<p>Let’s dive a little deeper and systematically explore this relationship to find the optimal training duration.</p>
<div id="29de6d37" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot the f1 score as a function of the number of epochs</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>epoch_options <span class="op">=</span> [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">2000</span>, <span class="dv">5000</span>, <span class="dv">10000</span>, <span class="dv">15000</span>]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> []</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> epoch_options:</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    model_nn <span class="op">=</span> NeuralNetwork(tolerance <span class="op">=</span> <span class="dv">0</span>, max_epochs<span class="op">=</span>epoch)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    model_nn.fit(X_train, y_train_binary, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    results_nn <span class="op">=</span> evaluate_model(model_nn, X_train, y_train_binary, X_test, y_test_binary)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    f1_scores.append(results_nn[<span class="st">'test_f1'</span>])</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(style<span class="op">=</span><span class="st">"whitegrid"</span>, palette<span class="op">=</span><span class="st">"colorblind"</span>, font_scale<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_options, f1_scores)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'F1 Score'</span>)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'F1 Score vs Number of Epochs'</span>)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The F1 score may still be improving, so let’s test longer training durations. While we’re at it, let’s also see how the other performance metrics change. Additionally, I’ll also examine the relationship between training time and epochs to understand the computational cost.</p>
<div id="5274d3ae" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot the f1 score as a function of the number of epochs</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>epoch_options <span class="op">=</span> [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">2000</span>, <span class="dv">5000</span>, <span class="dv">10000</span>, <span class="dv">15000</span>, <span class="dv">20000</span>, <span class="dv">30000</span>, <span class="dv">50000</span>, <span class="dv">100000</span>]</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>time_taken <span class="op">=</span> []</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> []</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>accuracy_scores <span class="op">=</span> []</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>precision_scores <span class="op">=</span> []</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>recall_scores <span class="op">=</span> []</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> epoch_options:</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    model_nn <span class="op">=</span> NeuralNetwork(tolerance <span class="op">=</span> <span class="dv">0</span>, learning_rate <span class="op">=</span> <span class="fl">0.2</span>, max_epochs<span class="op">=</span>epoch)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time()</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    model_nn.fit(X_train, y_train_binary, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time()</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    time_taken.append(end_time <span class="op">-</span> start_time)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    results_nn <span class="op">=</span> evaluate_model(model_nn, X_train, y_train_binary, X_test, y_test_binary)</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    f1_scores.append(results_nn[<span class="st">'test_f1'</span>])</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    accuracy_scores.append(results_nn[<span class="st">'test_accuracy'</span>])</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    precision_scores.append(results_nn[<span class="st">'test_precision'</span>])</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    recall_scores.append(results_nn[<span class="st">'test_recall'</span>])</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Plot 1: Time Taken vs Epochs ---</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_options, time_taken, marker<span class="op">=</span><span class="st">'o'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'teal'</span>)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Time Taken (s)'</span>)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Time vs Number of Epochs'</span>)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Plot 2: Evaluation Metrics vs Epochs ---</span></span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_options, f1_scores, marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'F1 Score'</span>)</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_options, accuracy_scores, marker<span class="op">=</span><span class="st">'s'</span>, label<span class="op">=</span><span class="st">'Accuracy'</span>)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_options, precision_scores, marker<span class="op">=</span><span class="st">'^'</span>, label<span class="op">=</span><span class="st">'Precision'</span>)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_options, recall_scores, marker<span class="op">=</span><span class="st">'D'</span>, label<span class="op">=</span><span class="st">'Recall'</span>)</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Score'</span>)</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Metrics vs Number of Epochs'</span>)</span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>plt.legend(title<span class="op">=</span><span class="st">'Metrics'</span>, loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="insights" class="level4">
<h4 class="anchored" data-anchor-id="insights">Insights</h4>
<p><strong>Training Time vs.&nbsp;Epochs</strong>: As expected, training time scales linearly with epochs. This makes sense as each epoch requires one complete forward and backward pass through the entire dataset.</p>
<p><strong>Performance Metrics</strong>: - The F1 score actually seems to peak around 15,000 epochs and then begins to decline steadily. - The accuracy is highest at 100 epochs (roughly 80%). Recall that 80% of our wine is “low quality”. I’m guessing that this model is just predicting low quality for all wines. This is why other metrics are valuable to consider. - Precision and recall both follow the same trend as the F1 score since the F1 score is the harmonic mean of precision and recall.</p>
<p><strong>Learning for the Future</strong>: There are more sophisticated techniques for determining optimal training duration (early stopping, learning rate scheduling, etc.), but this systematic approach gives us valuable insights into our model’s behavior. Additionally, in retrospect, designing the NN class to output weights at sub-epoch intervals would have been more computationally efficient than retraining each time.</p>
<p>Let’s use the optimal 15,000-epoch model for our final analysis</p>
<div id="d18465a4" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>epoch_max_f1 <span class="op">=</span> epoch_options[f1_scores.index(<span class="bu">max</span>(f1_scores))]</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of epochs with max F1 score: </span><span class="sc">{</span>epoch_max_f1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Grab the model with the highest F1 score</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>model_nn <span class="op">=</span> NeuralNetwork(tolerance <span class="op">=</span> <span class="dv">0</span>, learning_rate <span class="op">=</span> <span class="fl">0.2</span>, max_epochs<span class="op">=</span>epoch_max_f1)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>model_nn.fit(X_train, y_train_binary)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>results_nn <span class="op">=</span> evaluate_model(model_nn, X_train, y_train_binary, X_test, y_test_binary, model_nn.loss_history)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of epochs with max F1 score: 50000</code></pre>
</div>
</div>
<div id="339fa3e4" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>results_nn <span class="op">=</span> evaluate_model(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    model_nn, X_train_logistic, y_train_binary, X_test_logistic, y_test_binary, model_nn.loss_history</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Neural Network Performance Summary:"</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Metric'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Training'</span><span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Test'</span><span class="sc">:&lt;12}</span><span class="ss">"</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Accuracy'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'train_accuracy'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'test_accuracy'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Precision'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'train_precision'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'test_precision'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Recall'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'train_recall'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'test_recall'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'F1-Score'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'train_f1'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'test_f1'</span>]<span class="sc">:&lt;12.4f}</span><span class="ss">"</span>)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Training Information:"</span>)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">45</span>)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Final Loss:'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'final_loss'</span>]<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Epochs Trained:'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'epochs_trained'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Avg Recent Loss:'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>results_nn[<span class="st">'avg_recent_loss'</span>]<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Neural Network Performance Summary:
---------------------------------------------
Metric          Training     Test        
---------------------------------------------
Accuracy        0.8634       0.7577      
Precision       0.7111       0.4071      
Recall          0.5233       0.6032      
F1-Score        0.6029       0.4861      

Training Information:
---------------------------------------------
Final Loss:          0.306974
Epochs Trained:      50000
Avg Recent Loss:     0.306986</code></pre>
</div>
</div>
<p>The accuracy dropped 2% but the F1 score increased by about 0.02. For our dataset, where the targets are not evenly distributed (80% of the wines are low quality), the F1 score provides a more meaningful evaluation metric. Unlike accuracy, which can be misinterpreted due to the majority class, F1 score balances precision and recall, offering a better reflection of model performance on both classes.</p>
</section>
</section>
<section id="model-comparison" class="level3">
<h3 class="anchored" data-anchor-id="model-comparison">Model Comparison</h3>
<p>One final analysis that I’m interested in is to compare the predictions of the neural network and logistic regression models. Let’s see how well they agree on the test set.</p>
<div id="5234b17e" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get model predictions and true labels</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>nn_test_proba <span class="op">=</span> results_nn[<span class="st">'y_test_proba'</span>].flatten()</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>lr_test_proba <span class="op">=</span> results_logistic[<span class="st">'y_test_proba'</span>].flatten()</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>quality_counts <span class="op">=</span> np.bincount(y_test_binary.flatten())</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>num_zeros <span class="op">=</span> quality_counts[<span class="dv">0</span>]</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>num_ones <span class="op">=</span> quality_counts[<span class="dv">1</span>]</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of poor quality wines: </span><span class="sc">{</span>num_zeros<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>num_zeros<span class="op">/</span><span class="bu">len</span>(y_test_binary)<span class="op">*</span><span class="dv">100</span><span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of good quality wines: </span><span class="sc">{</span>num_ones<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>num_ones<span class="op">/</span><span class="bu">len</span>(y_test_binary)<span class="op">*</span><span class="dv">100</span><span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap, BoundaryNorm</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> y_test_binary.flatten()</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> ListedColormap([<span class="st">'#1f77b4'</span>, <span class="st">'#d62728'</span>])  <span class="co"># blue, red</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>bounds <span class="op">=</span> [<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">1.5</span>]  <span class="co"># so that 0 maps to blue, 1 to red</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>norm <span class="op">=</span> BoundaryNorm(bounds, cmap.N)</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> plt.scatter(</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>    lr_test_proba,</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>    nn_test_proba,</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span>labels,</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>    cmap<span class="op">=</span>cmap,</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>    norm<span class="op">=</span>norm,</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">40</span>,</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>    edgecolors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>    linewidths<span class="op">=</span><span class="fl">0.5</span></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">'k--'</span>, label<span class="op">=</span><span class="st">'Perfect Agreement'</span>)</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Neural Network vs Logistic Regression Predictions"</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Logistic Regression Probability"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Neural Network Probability"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a>cbar <span class="op">=</span> plt.colorbar(scatter, ticks<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>cbar.ax.set_yticklabels([<span class="st">'Low Quality'</span>, <span class="st">'High Quality'</span>])</span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Number of poor quality wines: 1053 - 81.0%
Number of good quality wines: 247 - 19.0%</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-22-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It appears that the neural network and logistic regression models agree more on the high quality wines as indicated by the cluster of data in the top right corner of the scatter plot. We can also note that the neural network is more conservative in its predictions since nearly all the points are below the diagonal. This is in alignment with the logistic regression model’s recall (85%) and precision (24%) - suggesting that it frequently predicts many false positives.</p>
</section>
</section>
<section id="conclusion-what-weve-learned" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-what-weve-learned">Conclusion: What We’ve Learned</h2>
<p>As we wrap up this exploration, I’m struck by how much ground we’ve covered — not just in understanding and predicting wine quality, but in learning the fundamentals of machine learning through this hands-on experience.</p>
<section id="model-performance-summary" class="level3">
<h3 class="anchored" data-anchor-id="model-performance-summary">Model Performance Summary</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td>~0%</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr class="even">
<td>Logistic Regression</td>
<td>46%</td>
<td>0.24</td>
<td>0.85</td>
<td>0.38</td>
</tr>
<tr class="odd">
<td>Neural Network</td>
<td>77%</td>
<td>0.41</td>
<td>0.53</td>
<td>0.46</td>
</tr>
</tbody>
</table>
</section>
<section id="key-learnings" class="level3">
<h3 class="anchored" data-anchor-id="key-learnings">Key Learnings</h3>
<p><strong>On Wine Quality</strong> - There is a genuine relationship between wine quality and the chemical properties of the wine. - Alcohol content emerged as the strongest predictor of wine quality - higher alcohol wines tend to be rated more favorably. - Quality is not dictated by a single feature, but by the interplay between multiple chemical properties.</p>
<p><strong>On Machine Learning</strong> - Incorporating all 11 chemical features significantly improved predictive performance — exactly what we’d expect in a multidimensional problem. - Neural networks require careful tuning. There are many parameters to consider (learning rate, number of epochs, the architecture of the network, etc.). Building these models requires careful thought and iteration. - Evaluating the performance of the model is not as straightforward as it seems. How accurately the model predicts the target variable in the test set is not enough. Other metrics must be considered to gain a fuller picture.</p>
</section>
<section id="next-steps-and-future-explorations" class="level3">
<h3 class="anchored" data-anchor-id="next-steps-and-future-explorations">Next Steps and Future Explorations</h3>
<p>Throughout this project, specifically while coding the neural network, there were several things I was curious to try. Specifically:</p>
<ul>
<li>Changing the model architecture - More layers, different activation functions, different neuron counts, etc.</li>
<li>Playing with the learning rate - specifically using algorithms such as Adam.</li>
<li>Exploring how stochastic gradient descent (SGD) compares to full batch gradient descent both in resultant model quality and training time.</li>
</ul>
<p>Thanks for reading!</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-wine_quality_186" class="csl-entry" role="listitem">
Cortez, Cerdeira, Paulo, and J. Reis. 2009. <span>“<span>Wine Quality</span>.”</span> UCI Machine Learning Repository.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>